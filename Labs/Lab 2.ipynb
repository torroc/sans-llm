{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d9877102",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\pagenumbering{gobble}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba88514",
   "metadata": {},
   "source": [
    "# Lab 2: Exploring Ollama\n",
    "\n",
    "## Overview\n",
    "Many of our labs will require the use of an LLM. Rather than using an online hosted commercial LLM with all of the associated fees, we will use a containerized version of Ollama serving a 3 billion parameter model.\n",
    "\n",
    "## Goals\n",
    "\n",
    " * Pull and serve the Llama3 model in an Ollama container.\n",
    " * Understand how to use the HTTP API to interact with the model.\n",
    " * Develop an understanding of how context drives chatbots.\n",
    "\n",
    "## Estimated Time: 60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd729486",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.1\n",
    "\n",
    "## Pulling a Trained Model\n",
    "\n",
    "When you issued the initial `docker compose up` command, several different services were started. One of those is the Jupyter system through which you are interacting with these exercises. Another set of services supports a *vector store* that we will be using later in the class. The last is a container that is hosting and serving Ollama.\n",
    "\n",
    "Ollama is an open source project under the MIT license design to host and serve various open LLMs. In this course, we will make use of the `llama3` model, but feel free to experiment with any of them. We have chosen this particular model because it is small enough to fit within the contraints of the system requirements that were specified for this course.\n",
    "\n",
    "When the container is first started, it is ready to work but has no model loaded. We need to interact with it a bit to instruct Ollama to `pull` the `llama3` model. To do this we will interact with the HTTP API that it provides on port 11434.\n",
    "\n",
    "To begin with, we need to load some Python libraries so that we can issue HTTP API calls easily. Please import the `requests` and `json` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28985ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e924083",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.2\n",
    "\n",
    "Let's start by verifying that Ollama is reachable. Normally we would need to either have Ollama running locally, know its IP address, or know its fully qualified domain name. In our cases, since we are running all of these containers together, we can take advantage of the automatic naming that containerization solutions provide.\n",
    "\n",
    "Using the host name `ollama`, send an HTTP GET request to that host on port `11434` and examine the returned content. This can be done using the `requests.get()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129f1f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1d512fe-9baa-41db-9a67-a878877554ab",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.3\n",
    "\n",
    "The `Ollama is running` response tells us that Ollama is up and running, however there's still another step that must be taken. Ollama provides a platform that can download and serve a number of different models. While Ollama is running, we have not downloaded any models.\n",
    "\n",
    "Let's prove this and demonstrate how to send data to the API. Since we are sending data, we must use an HTTP `POST` rather than a `GET`, which we just did. To send a `POST` we can use `requests.post()`. This will, however, require a few more arguments:\n",
    "\n",
    " * We must configure the *request headers* to specify the data type we are sending and that we wish to receive. This should be a dictionary containing `{'Content-Type':'application/json'}`.\n",
    " * We must also send a JSON body. This can also be built as a Python dictionary with the following keys and values:\n",
    "   - A `model` key with the value `llama3`, which is the model we wish to use. This parameter allows us to select the model used to generate responses.\n",
    "   - A `prompt` key with the text or prompt we want completed. Let's use `What is 42?`.\n",
    "   - A `stream` key with the value `False`. This key allows us to control whether the response is returned as a single response or streamed as individual tokens are generated by the model. For now, let's be patient and wait for the entire response.\n",
    "\n",
    "Our request must also be sent to a different API endpoint. To ask a model to generate text, the URL we must use is `http://ollama:11434/api/generate`.\n",
    "\n",
    "Please use the empty cell below to generate a `POST` request to the Ollama container. Use the `requests.post()` method, passing the URL, the headers and the data. The headers should be passed using kwarg `headers` and the data should be sent using kwarg `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871fe1c-0eb0-4f6e-aa05-cee662023dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fe89f96-81b4-48f8-b476-ba4aa8c853c9",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.4\n",
    "\n",
    "As predicted, the server reports that we do not have a model loaded with the message, `b'{\"error\":\"model \\\\\"llama3\\\\\" not found, try pulling it first\"}'`.\n",
    "\n",
    "To tell Ollama to pull the model, we must use the `/api/pull` API endpoint. To use this endpoint, we must configure the data that we send with the name of the model to pull.\n",
    "\n",
    "Use the following cell to send a `POST` request to the `/api/pull` endpoint. This time, the `data` parameter should be configured as:\n",
    "\n",
    "`data = {\"name\":\"llama3\", \"stream\":False}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b37ea0-e864-45eb-b3cd-ac6f7541f509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f18490d-179f-4123-958a-fa624ebce1e2",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.5\n",
    "\n",
    "Running this cell will require some patience. In fact, if you watch the command line from which you ran `docker compose up` you will see Ollama messages detailing the download progress. Please be patient. Depending on your Internet connection speed, this could take several minutes to complete. Once it does complete, you should see the final status message indicating `\"success\"`.\n",
    "\n",
    "With the model now downloaded, we should be able to send a query. Please resend the same request from **Task 2.3**. Capture the `.content` of the request in a variable named `response`. (For example, `response = requests.post(URL).content`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312c1676-696c-40b1-bdc6-be639729cabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf7d3247-ea23-47fe-b7af-c1cf6506dcfd",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.6\n",
    "\n",
    "This cell may take 30 seconds or more to run. When it completes, you will see the asterisk turn into a number, indicating completion, but you should not see any output since we captured the content of the result into a variable. Please execute the following cell to examine the content returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc91529-49f1-4dad-a8be-f497775077e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df1a75a8-8e35-4965-ab50-e792d8efa4f1",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.7\n",
    "\n",
    "First, the exact response that you receive may be different from the result shown above in the solutions notebook. This is because there is a bit of randomness added to the next-word result in the model. Take a few moments to examine the response. You should be able to find the following things:\n",
    "\n",
    " * `model` key, indicating this result is from `llama3`.\n",
    " * `created_at` key, telling you when the response was generated.\n",
    " * `response` key, providing the complete response as a string.\n",
    " * `done` key, indicating that the response is complete.\n",
    " * `done_reason` key, telling us why the model stopped.\n",
    " * `context` key, providing a list of the token indices including the prompt and the response.`\n",
    " * `total_duration` key, indicating the number of nanoseconds spent generating the resopnse.\n",
    " * `load_duration` key, indicating the number of nanoseconds spent loading the model.\n",
    " * `prompt_eval_count` key, the number of tokens in the prompt.\n",
    " * `prompt_eval_duration` key, the time in nanoseconds spent evaluating the prompt.\n",
    " * `eval_count` key, indicating the total number of tokens sent in the response.\n",
    " * `eval_duration` key, detailing the number of nanoseconds spent generating the response.\n",
    "\n",
    "To make the response easier to work with, let's convert it into a Python dictionary. This can be done using the `json.loads()` function.\n",
    "\n",
    "Use the next cell to decode the `response` into a Python dictionary named `response`. Once this is done, print out the `'response'` key from this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeac5e9-bb34-48de-a5cd-6dec77200002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6d9c681-51ee-43f7-8812-84d5ec77d12e",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.8\n",
    "\n",
    "Wonderful! While the response is not fast, we are able to send queries and have them answered. What about speed?\n",
    "\n",
    "First, we will make no attempt to speed things up in this class. The reason that the model response seems slow is twofold. First, we have not done anything to attempt to get any GPUs in the system properly configured, nor have we attempted (nor will we) to install GPU driver support into Docker or Kubernetes. If your organization is planning to deploy this type of model you should *definitely* investigate which GPUs make the most sense for your applications, your platforms (containerized or not), and your systems.\n",
    "\n",
    "The second reason this seems so slow is that we do not see anything until the entire response has been generated. To improve our experience during class (and for any interactive chat app you might build), let's change how we're making the request.\n",
    "\n",
    "The `\"stream\"` option in the JSON request, when set to `True`, will stream chunks of the response (tokens) as they become available. This sounds much more pleasant, but it requires a bit of a different approach in our Python code.\n",
    "\n",
    "Please consider the Python code in the cell below and, when you have a good handle on what it is doing, execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac85495-7717-4b2d-b9e8-eab70f50f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(url, data):\n",
    "    session = requests.Session()\n",
    "\n",
    "    with session.post(url, data=data, stream=True) as resp:\n",
    "        for line in resp.iter_lines():\n",
    "            if line:\n",
    "                token = json.loads(line)[\"response\"]\n",
    "                print(token, end='')\n",
    "\n",
    "data = {\"model\":\"llama3\", \"prompt\": \"Which LLM are you?\", \"stream\":True}\n",
    "url = 'http://ollama:11434/api/generate'\n",
    "\n",
    "get_stream(url, json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5131380-0311-4cbd-ac6b-7a354ba34566",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.9\n",
    "\n",
    "Wow, that's much better! It's still taking the model a while to generate the answer, but the delay is much more tolerable since we can see what it is doing. Before concluding this lab, let's investigate the `\"context\"` value and see how it can be used. First:\n",
    "\n",
    "Using the next cell and the techniques above, ask the model, \"Who was Macbeth?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a0cf6-4595-4b66-8552-eab768df865b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86f7aad9-f4a0-4138-b688-457ba5300302",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.10\n",
    "\n",
    "That response seems completely reasonable. In the event you are looking at the solution while working through this on your own, do not be concerned if the response generated by your model is not identical. No doubt it includes the highlights; specifically, something about Macbeth being a fictional Shakespearean character based on a real historical figure.\n",
    "\n",
    "Using the next cell and the techniques above, ask the model, \"What did the witches say about him?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ebd23-ce9a-47c2-b7d7-b8f5ba02ed90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4e454e-8440-4f5b-8e3f-490468f2df1b",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.11\n",
    "\n",
    "What happened? The model acts like it has no idea what we are talking about!\n",
    "\n",
    "The problem is that every prompt that we send to the model is viewed as a completely discrete event. Unless we do something to remind the model about the history of our conversation, it will have no way to connect the second question to the first, resulting in a response that isn't particularly useful. This brings us to the `\"context\"` field.\n",
    "\n",
    "The context is a list of tokens that the model returns to us, providing us information in the form of token numbers about the question and the response that the model generates. If we store this value from the response to our first question and then send it in our second question, the model will perform as we might expect. Let's try it.\n",
    "\n",
    " * Redefine the `get_stream()` function such that it returns the `'context'` array from the JSON object in the last part of the stream.\n",
    " * Capture this value in a variable\n",
    " * Use this new function to re-send the question, \"Who was Macbeth?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37b65e-1c94-4c24-b33d-f651f1e761c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "062aa71e-9b6d-4383-9024-eadee4ff591f",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.12\n",
    "\n",
    "Now that we have the initial answer and the context, we are ready to ask the second question. We just have to remember to send the context value in the `data` object.\n",
    "\n",
    " * Add a `\"context\"` key to the `data` dictionary with the context array that was returned in the last cell.\n",
    " * Ask the model, \"What did the witches say about him?\", sending the context in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7ed9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "083563be-8a6b-462a-b6b8-11436f80e80f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab we have accomplished some important things and learned some useful techniques:\n",
    "\n",
    " * We now have the Llama 3 model installed in our Ollama container.\n",
    " * We know how to interact with the API to pull models.\n",
    " * We know how to interact with the API to send questions.\n",
    " * We understand the function of the `stream` attribute and have code that allows us to receive and print out each part of the response as it arrives.\n",
    " * We understand how the `context` is returned and how it can be included in a subsequent query to continue the \"conversation.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
