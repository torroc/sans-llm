{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9f5288c3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\pagenumbering{gobble}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba88514",
   "metadata": {},
   "source": [
    "# Lab 3: Retrieval Augmented Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "Now that we have an understanding of tokenization, vectorization, and can interact with our local model, we will turn our attention to leveraging that model to obtain useful responses. To do this, we need to understand how to preprocess our data effectively and make use of a vector database. In this lab we will do just this; preprocess input data into chunks, vectorize the data, store the vectors and metadata into a vector database, and then leverage that database to create useful input to the LLM with the hope of generating a useful and meaningful response.\n",
    "\n",
    "## Goals\n",
    "By the end of this lab you should know how or be able to:\n",
    "\n",
    " * Preprocess PDF documents.\n",
    " * Generate text embeddings for the chunks.\n",
    " * Store the embeddings into a vector database along with useful metadata.\n",
    " * Query the vector database for stored vectors nearest a user question.\n",
    " * Create a simple, but effective, prompt that provides guardrails on the model's response and ensures the response has no \"hallucinations.\"\n",
    "\n",
    "## Estimated Time: 60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99204f",
   "metadata": {},
   "source": [
    "# On Fine Tuning\n",
    "\n",
    "There are a number of libraries and a large number of discussions available on the finetuning of language models. Finetuning the model can be accomplished in two main ways:\n",
    "\n",
    " * Train the entire model.\n",
    " * Freeze the model, add adapter layers and train the adapter layers only.\n",
    "\n",
    "## Finetuning the Entire Model\n",
    "\n",
    "Tuning the entire model seems very cool but has a number of drawbacks. First, you must have sufficient compute and memory capability to load the entire model and at least one batch of training data. Given the sizes of even the smallest of LLMs, you would either need to purchase at least one nVidia A100 40gb GPU for something around 8,000 dollars or you would rent time on a cloud based GPU for more than $2 per hour (for a 40gb A100... A single cloud based 80gb H100 rents for nearly 5 dollars per hour), which extends to between 1,500 and 3,700 dollars for the GPU rental alone, which does not include instance, IO, storage, and other costs. Still, if you only need to fine tune the model once, this might make good financial sense. Unfortunately, this is only the first drawback.\n",
    "\n",
    "Not only is finetuning the entire model costly, it is also time consuming. Fine tuning will typically take at least hours but far more likely days, or even many days. If you are attempting to fine tune a model on consumer grade hardware, you would likely choose to go the route of LoRA (Low Rank Adaptation) or QLoRA (Quantized Low Rank Adaption), which are strategies for vastly reducing the memory footprint required for a model by sacrificing overall accuracy. Essentially, rather than representing a floating point value as something like $3.981237871$, a low rank variant (think \"fewer bits\") might be $3.981$. While this takes far less memory, we've also lost a significant number of digits. In LoRA this low rank approximation is applied to the weights matrices within the model, reducing its overall size. QLoRA goes a step further, additionally reducing the number of bits of precision in the adapter matrices (the weights within the layers added to the model to accommodate fine tuning).\n",
    "\n",
    "Another drawback is that if we choose to allow the entire model to be finetuned, we run the very real risk of *catastrophic forgetting*. What this means is that our training data might end up adjusting some of the weights within the pretrained model in such a way as to create a sort of \"domino effect\" of cascading failures, resulting in a model that is effectively unusable.\n",
    "\n",
    "To prevent this from happening, it is more common to *freeze* (prevent any of the trained paramters in the LLM from being updated during training) the pretrained model and add \"adapter\" layers to it, then train the adapter layers. While this is not truly accurate, you can think about it as adding a couple of layers that take care of nudging the LLMs output closer to something matching our training data or how we would like it to respond. For example, perhaps we would like the LLM to respond to the question, \"Who created you?\" with our own company named rather than something like \"Meta.\" If we were to include that type of question in our fine tuning data, the adapter layers would learn to steer the answer, sort of filtering it, toward the answer we desire.\n",
    "\n",
    "The final drawback that, at least to me, is very significant is that in all of these cases the model is still prone to the ill-termed behavior of *hallucination.* Since the model truly is just predicing the next most likely token based on the preceding context, this overly anthropomorphizes the model, creating the impression that it is thinking. In reality, the model is simply generating a bad and likely false response. Often people will try hard to compensate for this with long and arduous prompt engineering, but there is a better way.\n",
    "\n",
    "> We will include a solution notebook that demonstrates how to do finetuning should you wish to take that path, but we do not intend to cover finetuning in any additional detail.\n",
    "\n",
    "## RAG over Finetuning\n",
    "\n",
    "Even with finetuning, we have no real way to prevent the model from occasionally generating answers that are just wrong. Still, how can we get the model to give answers based on our data or some specific set of data? *Retrieval Augmented Generation*, or RAG, is the answer. Using this approach with some very simple prompt engineering we can leverage the model for what it is good at (text summarization) and word or sentence embeddings for what they are good at (locating thoughts or phrases that are similar to or related to the question posed). Let's dive in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd729486",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.1\n",
    "\n",
    "We will need various libraries to complete the lab ahead. Specifically, we need to import:\n",
    "\n",
    "* `json`, a Python library for encoding and decoding JSON objects.\n",
    "* `requests`, a Python library for HTTP based interactions.\n",
    "* From `sentence_transformers`, a Python library from Huggingface that implements the SBERT (Sentence based Bidirectional Encoder Representations from Transformers) library, the `SentenceTransformer` class. We will use this for our embeddings generation.\n",
    "* `PdfReader` from `pypdf`, a PDF parsing and processing library.\n",
    "* `RecursiveCharacterTextSplitter` from `langchain_text_splitters`, a comprehensive library for building text processing pipelines for LLMs.\n",
    "* `Path` from `pathlib`. The `pathlib` library is a modern set of methods and classes for manipulating file paths.\n",
    "\n",
    "Please import these libraries using the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70bd4ea2-cd01-46c8-9ca0-97a296f463e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-11-25 17:48:46.128570: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 17:48:46.136448: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-25 17:48:46.145836: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-25 17:48:46.148648: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 17:48:46.155700: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 17:48:46.694434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d687dc2",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.2\n",
    "\n",
    "In our first lab we learned about word embeddings and how they are trained. We understand that these embeddings are developed by attempting to predict either the context of a word or the word based on the context. These embeddings can then be used in the place of the word and carry some type of semantic meaning.\n",
    "\n",
    "What we need to do is extend this notion of a word embedding to something larger, like a sentence or a paragraph. Think about why this is the case. Using word embeddings, we saw that it was possible to do arithmetic using the embedding vectors to find other words. What if we were able to generate a vector that captures important characteristics about sentences and paragraphs? Shouldn't it be possible to use some kind of math to find other vectors that are close to that chunk of text? Yes!\n",
    "\n",
    "There are a number of different approaches used to generate embeddings for larger pieces of text. The simplest of these might do something like taking the arithmetic mean (average) of all the vectors for all the words that make up that chunk. While this does work to a degree, as you can imagine it's not the best way to capture the overall meaning of a sentence.\n",
    "\n",
    "We will use a pre-trained set of embeddings known as SBERT (Sentence Bidirectional Encoder Representations from Transformers). While the training of SBERT embeddings is a bit more complicated than the word embeddings that we worked with, the overall notion is exactly the same. Since these embeddings are generated from chunks of text rather than words, they also tend to be much better than taking a simple average of word embeddings.\n",
    "\n",
    "The `sentence-transformers` library makes a number of different SBERT models available:\n",
    "\n",
    "| Model Name | Size |\n",
    "|------------|--------|\n",
    "| all-mpnet-base-v2 | 420 MB | \n",
    "| multi-qa-mpnet-base-dot-v1 | 420 MB | \n",
    "| all-distilroberta-v1 | 290 MB | \n",
    "| all-MiniLM-L12-v2 | 120 MB | \n",
    "| multi-qa-distilbert-cos-v1 | 250 MB | \n",
    "| all-MiniLM-L6-v2 | 80 MB | \n",
    "| multi-qa-MiniLM-L6-cos-v1 | 80 MB | \n",
    "| paraphrase-multilingual-mpnet-base-v2 | 970 MB | \n",
    "| paraphrase-albert-small-v2 | 43 MB | \n",
    "| paraphrase-multilingual-MiniLM-L12-v2 | 420 MB | \n",
    "| paraphrase-MiniLM-L3-v2 | 61 MB | \n",
    "| distiluse-base-multilingual-cased-v1 | 480 MB | \n",
    "| distiluse-base-multilingual-cased-v2 | 480 MB | \n",
    "\n",
    "For our class, we will make use of the `multi-qa-distilbert-cos-v1` embeddings. This choice is a middle-of-the-road choice in terms of memory requirements, and it is a model trained using question and answer pairs (that's what the `qa` part of the name means). What about the `cos` part? This stands for Cosine and indicates that the vectors in this model have a very specific property; they are all scaled to be of length 1 in the embedding space.\n",
    "\n",
    "What does this mean for us? It tells us that this model is intended to be used for text similarity searches such as Cosine Similarity (image this to mean, \"Are these two vectors pointing in the same direction?\", Euclidean Distance (typically used in K-Nearest Neighbors types of searches), and Dot Product (which is a specific Linear Algebra operation that literally determines whether the vectors are pointing in the same direction). Are there other types of embeddings available? Yes. The most common \"other\" form is the `dot` embeddings. While these can be used for the Dot Product similarity test, they are not as useful for Euclidean distance types of search or Cosine Similarity, which is a very typical way to search for similar vectors. While you can choose to use the `-dot-` versions of the model, our experience is that the `-cos-` models function better with various similarity searches.\n",
    "\n",
    "Using the next cell, instantiate a `SentenceTransformer()` object, passing in `'sentence-transformers/multi-qa-distilbert-cos-v1'` as an argument. This will download and initialize the DistilBERT SBERT model for our use. Assign this model to a variable named `model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc7bbbe-9b26-4973-8762-e4e84c56ec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6b70b-0c91-45e2-a169-07690f719f55",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.3\n",
    "\n",
    "Let's experiment with the model so that we have an understanding of what it does. Using the next cell, create a list of strings named `strings`. Assign to this list the following strings:\n",
    "\n",
    " * The sky is blue today.\n",
    " * Machine learning uses mathematics to find patterns in data.\n",
    " * Neural networks are trained using backpropagation.\n",
    " * Backpropagation is the training method used to update neurons.\n",
    "\n",
    "Use the `model.encode()` method to encode the first string. Print out the embedding that is returned and the `shape` of that embedding (i.e., the `.shape` attribute of the value returned from the call to `encode()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431cdc8f-4010-4f27-a076-2e9c90d87e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.81845890e-02, -1.67494025e-02,  4.93362406e-03,  2.52056643e-02,\n",
       "         9.95677989e-03,  4.09304202e-02, -9.28256835e-04,  1.05975002e-01,\n",
       "        -6.20508417e-02, -4.82722744e-02, -1.86031237e-02, -5.28222397e-02,\n",
       "        -5.58008887e-02,  6.68877363e-02, -1.46532189e-02,  5.65091111e-02,\n",
       "        -2.10039206e-02, -2.24468708e-02,  2.78438423e-02, -7.09581189e-03,\n",
       "         6.86126016e-03,  1.24178827e-02, -8.08446016e-03, -1.17118619e-02,\n",
       "         3.57014760e-02, -3.37228775e-02, -7.50888288e-02,  3.78814787e-02,\n",
       "         1.08273495e-02,  2.81119402e-02, -3.63185816e-02, -2.31127851e-02,\n",
       "         2.02871747e-02,  2.48177797e-02, -8.47086590e-03, -1.15900170e-02,\n",
       "        -6.84459135e-02, -1.25005227e-02, -3.09792329e-02,  1.63190458e-02,\n",
       "         3.28300633e-02, -3.57349366e-02,  6.98456764e-02, -5.29162064e-02,\n",
       "         1.30857173e-02,  8.31280928e-03,  1.88670587e-02,  5.31996116e-02,\n",
       "         5.06243445e-02, -2.37999689e-02,  1.17153041e-02,  1.05823968e-02,\n",
       "        -1.55181903e-03, -3.90584841e-02, -4.63805348e-03,  3.08490098e-02,\n",
       "         2.55792448e-03,  1.64344930e-03, -8.30741366e-04,  2.25969274e-02,\n",
       "         6.72426373e-02, -7.44569600e-02, -2.40478255e-02, -4.70232181e-02,\n",
       "        -3.04182451e-02,  3.16447653e-02, -3.11304014e-02, -7.19421655e-02,\n",
       "         5.13651699e-04, -2.31227819e-02,  5.13687655e-02,  4.08559442e-02,\n",
       "         9.42943059e-03,  4.49797692e-04, -6.74601644e-02,  7.01324455e-03,\n",
       "        -2.91848872e-02,  3.29800919e-02, -2.17915811e-02, -6.17708415e-02,\n",
       "         5.09262234e-02,  6.84765354e-02, -1.13684917e-02,  1.96749009e-02,\n",
       "        -6.32126257e-02,  2.00657826e-02, -6.18508086e-03,  3.38039473e-02,\n",
       "         6.78803539e-03,  4.17749658e-02, -1.64683852e-02, -1.88787724e-03,\n",
       "         1.88987739e-02, -1.91449411e-02, -8.58892724e-02,  1.10043613e-02,\n",
       "        -4.62878682e-02, -1.87015627e-02,  5.62378243e-02, -3.70389149e-02,\n",
       "        -1.02146517e-03, -1.80525538e-02,  8.85016397e-02,  7.87317008e-02,\n",
       "        -3.42505565e-03, -2.58246083e-02,  2.90920818e-03, -3.76355089e-02,\n",
       "         4.62640636e-03,  4.11439463e-02,  4.90112789e-02, -2.65545268e-02,\n",
       "         1.15453685e-02,  8.86080693e-03,  2.77117975e-02,  2.08143797e-02,\n",
       "         2.96635516e-02, -3.79607230e-02,  4.17149335e-04, -7.94307590e-02,\n",
       "        -3.46135944e-02, -4.38710471e-04, -3.13625224e-02,  1.40734902e-02,\n",
       "        -4.74964306e-02, -1.27953179e-02, -1.29268952e-02, -2.84782574e-02,\n",
       "        -9.69979819e-03, -2.25375500e-02,  7.39644021e-02, -2.07001250e-02,\n",
       "        -5.08253947e-02, -6.60695229e-03,  8.96486118e-02,  3.54043990e-02,\n",
       "         3.17665711e-02,  6.89291907e-03, -5.21382429e-02,  3.02401725e-02,\n",
       "         1.95015816e-03,  3.12134065e-02, -9.39868670e-03,  1.71246380e-02,\n",
       "        -2.49659941e-02,  6.18637353e-02,  7.81948492e-02,  7.61602959e-03,\n",
       "         2.58103851e-02, -8.21084976e-02,  2.68275328e-02, -1.84987225e-02,\n",
       "         8.96673091e-03,  1.03469782e-01,  4.57451195e-02, -3.67160246e-05,\n",
       "        -7.81848654e-02, -3.08272578e-02,  1.94040760e-02,  1.10418955e-02,\n",
       "         6.62923753e-02,  1.80982407e-02,  4.57193628e-02,  3.07125840e-02,\n",
       "        -1.38457492e-02,  6.33147825e-03, -7.77760847e-03, -5.98051399e-02,\n",
       "         2.94140931e-02,  1.36355935e-02, -1.89384818e-02, -5.67781068e-02,\n",
       "         9.79967508e-03,  1.77004207e-02, -1.82511900e-02, -1.72279943e-02,\n",
       "        -5.84612787e-03, -5.75359038e-04, -3.98569033e-02,  1.38761681e-02,\n",
       "        -3.11648343e-02, -3.13225598e-03, -4.81520109e-02, -1.19243665e-02,\n",
       "         1.51572647e-02,  3.36797535e-02,  1.45594366e-02, -1.37207634e-03,\n",
       "         5.81636131e-02, -5.16698658e-02, -2.96407845e-02, -2.19137613e-02,\n",
       "         8.61697458e-03,  9.96539835e-03, -2.48416923e-02, -5.54693006e-02,\n",
       "         2.70853303e-02, -2.49073822e-02, -6.41825469e-03,  4.78990981e-03,\n",
       "         5.56363948e-02,  1.22640133e-02, -3.38223279e-02, -3.23630050e-02,\n",
       "         2.84443907e-02,  1.12677384e-02, -5.02473395e-03,  8.64042621e-03,\n",
       "        -6.38127401e-02,  5.34042455e-02, -5.46358339e-02,  2.34611593e-02,\n",
       "         5.96457720e-02,  7.27898031e-02, -1.56872645e-02, -7.66530028e-03,\n",
       "         7.22736353e-03, -4.51649837e-02, -1.45491874e-02, -1.30372634e-03,\n",
       "        -6.03894629e-02,  1.01200633e-01,  6.07074099e-03, -4.10775328e-03,\n",
       "         3.27632912e-02, -3.94647270e-02,  1.35804964e-02, -1.61909461e-02,\n",
       "        -5.99571038e-03,  1.31655158e-02,  2.32505742e-02, -3.26679014e-02,\n",
       "        -6.27338607e-03,  2.16738507e-02,  3.60561125e-02, -1.18616866e-02,\n",
       "         3.59475501e-02, -2.53555495e-02,  3.75415827e-03,  2.53562164e-02,\n",
       "         1.84783526e-02, -9.13517103e-02, -2.13512611e-02,  2.30855998e-02,\n",
       "        -2.76469328e-02,  1.37216346e-02,  8.00015312e-03,  2.76002567e-03,\n",
       "        -6.45744335e-03, -2.70407777e-02, -1.26409298e-02,  8.05501360e-03,\n",
       "        -4.24953029e-02,  2.42085624e-02, -5.05379885e-02,  3.79348397e-02,\n",
       "        -5.27454019e-02,  4.17720266e-02,  3.63319293e-02,  2.77366862e-02,\n",
       "        -2.00991184e-02, -3.84365320e-02,  2.85388697e-02, -1.25667667e-02,\n",
       "         6.25994522e-03,  1.16948215e-02, -3.15398574e-02,  4.32650000e-02,\n",
       "         1.22677605e-03, -8.16236213e-02,  5.26300669e-02,  1.37948757e-02,\n",
       "         3.23663875e-02,  1.66684911e-02,  1.87904779e-02,  2.35152375e-02,\n",
       "         1.03634195e-02, -5.94322244e-03, -4.43911031e-02, -2.92864498e-02,\n",
       "         2.93457527e-02, -1.82012711e-02, -4.97213416e-02,  2.10919753e-02,\n",
       "         4.35478128e-02, -6.42882735e-02,  1.90720670e-02, -1.24507667e-02,\n",
       "        -5.75305410e-02,  2.90934723e-02,  8.96069854e-02,  2.74333078e-02,\n",
       "        -4.00901772e-02, -1.64713338e-02,  1.35787698e-02,  3.34056243e-02,\n",
       "         4.49991599e-02, -2.82089841e-02, -1.93286706e-02, -5.13460673e-02,\n",
       "         6.18111193e-02, -1.44935856e-02,  5.51119819e-02, -1.05357943e-02,\n",
       "         2.22336687e-02, -5.68868294e-02,  3.14308191e-03,  5.39457165e-02,\n",
       "        -1.65822986e-03,  3.80949713e-02,  1.13546867e-02, -3.32367420e-02,\n",
       "         3.04838200e-03,  8.74022208e-03, -5.54917604e-02,  2.72936542e-02,\n",
       "         7.06873089e-02, -3.11902780e-02, -9.13857948e-03, -8.20798129e-02,\n",
       "        -1.07298708e-02,  2.05760263e-02, -5.58163412e-02, -3.52275558e-02,\n",
       "         3.42913084e-02, -4.14621942e-02,  1.61481742e-02,  6.88118860e-03,\n",
       "        -1.49259092e-02,  4.25522104e-02,  2.36249976e-02, -3.59149161e-03,\n",
       "        -6.30323067e-02, -6.13784008e-02,  2.04883534e-02,  7.62183312e-03,\n",
       "         4.45849113e-02,  1.76039748e-02,  7.86045752e-03, -3.19753662e-02,\n",
       "         6.17968328e-02,  4.97329002e-03,  5.29489927e-02,  3.27374451e-02,\n",
       "        -4.65272777e-02,  1.81360673e-02, -1.19401589e-02, -5.46375464e-04,\n",
       "        -3.84181179e-02, -5.19982614e-02, -2.22585928e-02, -1.47149861e-02,\n",
       "        -1.05362013e-02,  3.08259390e-02, -3.61230131e-03, -5.59517741e-02,\n",
       "         3.66658755e-02, -1.38856322e-02,  4.00126819e-03, -1.70866866e-03,\n",
       "         2.73431116e-03, -2.38934793e-02, -1.76447798e-02,  1.47946244e-02,\n",
       "         7.55672483e-03,  1.43913482e-03,  2.04065051e-02,  9.48300294e-04,\n",
       "         1.60425790e-02, -4.22626128e-03,  3.56992707e-02,  1.88481454e-02,\n",
       "         8.92377496e-02, -1.87953785e-02,  1.69001184e-02,  1.92668661e-02,\n",
       "        -1.16991866e-02,  7.97845144e-03, -1.18827820e-02, -2.62689777e-02,\n",
       "        -1.57763381e-02, -2.15182845e-02, -3.76872197e-02, -2.30616350e-02,\n",
       "        -2.21476108e-02, -3.17761488e-02,  3.21429595e-02,  3.66031341e-02,\n",
       "         2.44817529e-02, -4.34481129e-02,  5.05857058e-02,  1.55124683e-02,\n",
       "         3.99497163e-04,  2.09277906e-02, -4.06325124e-02,  1.46225896e-02,\n",
       "        -6.09539915e-04,  5.25092445e-02, -1.15499320e-02, -3.72353159e-02,\n",
       "        -1.28891105e-02,  7.88888708e-03, -2.11888482e-03,  5.85203581e-02,\n",
       "        -2.29923166e-02, -2.95765344e-02,  4.58458923e-02,  4.41459529e-02,\n",
       "         1.99562889e-02, -5.12035470e-03,  2.75173485e-02,  2.89980508e-02,\n",
       "        -8.24006181e-03, -1.25767961e-02,  6.17744401e-02,  1.71180144e-02,\n",
       "        -5.10848202e-02,  7.13862595e-04,  5.57953678e-02, -1.51929529e-02,\n",
       "        -1.80496983e-02, -8.62448942e-03,  4.30624932e-02,  2.35672668e-02,\n",
       "         1.00760706e-01,  1.31535353e-02, -9.15644038e-03,  2.14988701e-02,\n",
       "         2.35232096e-02,  2.02002544e-02,  1.48874186e-02,  4.55735549e-02,\n",
       "        -7.92354420e-02, -1.47862434e-02, -4.67069522e-02,  4.17197645e-02,\n",
       "        -2.43318523e-03, -2.03672890e-03,  2.56288182e-02,  3.32516730e-02,\n",
       "         4.46795337e-02, -1.77315772e-02,  4.34036180e-02, -2.47179512e-02,\n",
       "         4.92452085e-02, -6.67158663e-02, -1.25411516e-02,  5.40321739e-03,\n",
       "         3.12534766e-03,  3.37086543e-02,  2.70236749e-02,  5.80627471e-02,\n",
       "        -3.30632403e-02,  4.65033483e-03, -2.66713854e-02, -4.11293916e-02,\n",
       "         1.33182071e-02, -1.93223283e-02,  1.25545524e-02,  4.79399934e-02,\n",
       "        -4.59392965e-02, -2.40452420e-02, -2.23185401e-02,  3.22386622e-02,\n",
       "        -2.42396872e-02,  4.51193471e-03,  2.85049770e-02,  4.35424261e-02,\n",
       "        -4.55364492e-03, -1.64710246e-02, -3.16003636e-02,  2.64108311e-02,\n",
       "        -3.39402407e-02, -7.50451908e-03,  2.56331339e-02,  9.45148815e-04,\n",
       "         1.87638719e-02, -5.45702949e-02, -3.32793146e-02, -4.17113751e-02,\n",
       "        -4.13916493e-03,  4.55653332e-02, -1.56325735e-02,  5.75542748e-02,\n",
       "         4.27475497e-02, -5.68923503e-02,  2.41355151e-02, -8.92544445e-03,\n",
       "        -8.40890966e-03,  3.59652378e-02, -6.15253747e-02,  3.40052368e-03,\n",
       "        -2.17708619e-03,  6.41439855e-03,  2.40923911e-02, -1.51682533e-02,\n",
       "         2.41596419e-02,  3.56453024e-02,  5.77147044e-02, -3.09999026e-02,\n",
       "        -4.74938452e-02,  5.01730479e-03, -1.50442356e-03, -1.70409490e-04,\n",
       "         1.97885521e-02, -2.95618959e-02,  4.40321192e-02,  4.55047563e-02,\n",
       "        -3.20791267e-02,  3.94809246e-02,  2.86937095e-02, -9.48749669e-03,\n",
       "        -7.49299750e-02, -2.72911899e-02,  6.17870018e-02,  5.73530123e-02,\n",
       "        -8.71563181e-02,  6.48314832e-03, -4.45128372e-03, -3.85091715e-02,\n",
       "        -5.72362393e-02,  8.96718726e-03,  3.25187594e-02, -2.33779149e-03,\n",
       "         9.05843824e-03,  1.51722023e-04, -5.91399595e-02,  7.94675574e-03,\n",
       "        -4.95834462e-02,  4.64393422e-02, -1.91700887e-02, -9.89169162e-03,\n",
       "         4.27171439e-02,  3.37754451e-02, -3.45818214e-02,  2.61042360e-02,\n",
       "        -4.60838666e-03, -2.12384146e-02, -9.85912327e-03,  5.05464152e-02,\n",
       "        -1.69913173e-02,  4.65391614e-02, -1.57752447e-02,  1.29102860e-02,\n",
       "        -1.77589171e-02, -3.72578539e-02, -1.42636616e-02,  7.86298290e-02,\n",
       "         7.39357397e-02, -7.71505013e-02,  4.83580343e-02, -5.17692715e-02,\n",
       "        -2.74059623e-02,  2.62028854e-02, -1.69876926e-02,  3.36490907e-02,\n",
       "         4.53609973e-02, -4.43395264e-02, -6.79556355e-02, -2.80127004e-02,\n",
       "         2.37600673e-02,  2.58825030e-02,  6.34149322e-03, -6.53784052e-02,\n",
       "         1.82409829e-03,  4.29727547e-02, -2.22087670e-02, -1.26292361e-02,\n",
       "         3.29950675e-02, -1.07314155e-01, -3.34548987e-02,  2.57434731e-04,\n",
       "         1.40999248e-02,  2.12430339e-02,  1.25099448e-02,  2.53852550e-03,\n",
       "         5.91531955e-03,  2.99291871e-02, -8.07206705e-03, -3.63994315e-02,\n",
       "        -3.88461761e-02, -5.11029921e-02,  6.75172778e-03, -4.97770729e-03,\n",
       "        -1.03018759e-02, -4.33722837e-03, -4.31378894e-02,  6.44526724e-03,\n",
       "        -3.22256237e-02, -4.32374291e-02, -2.23887339e-02,  2.66610440e-02,\n",
       "        -3.14631984e-02, -1.97780374e-02,  4.25747372e-02,  4.49745096e-02,\n",
       "         2.01157611e-02, -4.81680967e-02,  6.47787675e-02,  4.36559245e-02,\n",
       "        -1.94037929e-02, -4.13938724e-02,  2.74602529e-02,  3.93300243e-02,\n",
       "        -4.74824198e-02,  1.67457405e-02, -1.22735482e-02, -3.30885090e-02,\n",
       "         3.81582715e-02,  2.48381197e-02,  2.58251675e-03, -2.41879025e-03,\n",
       "         2.55202614e-02,  1.70262400e-02,  6.40011504e-02, -5.29969446e-02,\n",
       "        -2.84956153e-02,  8.82076994e-02,  8.34128179e-04,  4.21197414e-02,\n",
       "        -1.07997181e-02,  4.57221679e-02, -2.85619870e-02,  1.88764501e-02,\n",
       "         4.01664004e-02, -1.98350977e-02, -1.63645726e-02,  6.81197941e-02,\n",
       "         1.28102638e-02, -4.27820310e-02, -3.52182798e-02,  2.19436213e-02,\n",
       "        -4.71832342e-02,  3.62283774e-02, -3.96373905e-02,  5.20266891e-02,\n",
       "         2.49202885e-02, -5.95493149e-03, -2.88527198e-02,  5.58029786e-02,\n",
       "         2.39700042e-02,  2.06134319e-02, -3.30633344e-03,  1.31085236e-02,\n",
       "        -1.98820066e-02, -4.25688289e-02, -1.83876287e-02, -6.10367656e-02,\n",
       "         4.89929728e-02, -1.70013253e-02,  4.45917025e-02, -3.07842009e-02,\n",
       "         3.80306132e-02, -2.08676439e-02, -3.61178294e-02,  1.35075869e-02,\n",
       "        -2.33764332e-02, -4.40486111e-02,  8.84757470e-03,  5.51067963e-02,\n",
       "         1.74765456e-02, -5.67093864e-02,  3.61457504e-02, -1.17132878e-02,\n",
       "        -4.53129895e-02, -3.52590308e-02, -4.87285443e-02, -8.40373058e-03,\n",
       "         2.00488940e-02,  3.26841734e-02,  1.74267553e-02, -3.49145159e-02,\n",
       "        -7.37400502e-02, -1.21683897e-02,  2.35034768e-02,  5.77167124e-02,\n",
       "        -1.40628098e-02, -7.27999359e-02, -1.20484279e-02, -5.86734572e-03,\n",
       "        -5.35771735e-02, -1.20462338e-02, -2.77605187e-03,  1.49066057e-02,\n",
       "        -3.60152274e-02, -2.48288400e-02,  3.55840959e-02,  2.28269901e-02,\n",
       "         7.48556387e-03,  9.78303049e-03, -9.75970402e-02, -1.05758887e-02,\n",
       "        -8.52822699e-03,  1.32304206e-02, -3.13597396e-02, -1.58954300e-02,\n",
       "        -1.69783253e-02,  5.10176420e-02, -9.34144761e-03, -3.11121456e-02,\n",
       "        -2.22848561e-02,  7.62273092e-03,  5.88456541e-02, -1.16564427e-02,\n",
       "        -2.40251683e-02,  1.22853899e-02, -4.73189913e-02,  4.62407172e-02,\n",
       "         4.83303703e-02, -5.81591018e-02, -4.28919159e-02,  9.93186887e-03,\n",
       "        -3.00122099e-03, -4.13355045e-02,  6.63138106e-02,  3.34724807e-03,\n",
       "         4.09177691e-02, -2.07477808e-03, -4.95507158e-02,  6.98800199e-03,\n",
       "        -1.86740812e-02, -2.45466735e-02,  3.00168078e-02, -4.53440547e-02,\n",
       "         6.64776936e-02,  2.35498343e-02,  2.57613901e-02, -4.84774038e-02,\n",
       "        -6.18514866e-02, -2.26686578e-02,  3.09087913e-02,  2.72008907e-02,\n",
       "         1.71440877e-02,  2.36756206e-02, -5.38499095e-02,  6.46460280e-02,\n",
       "        -1.03756385e-02, -2.26004738e-02,  2.19251215e-02, -1.68605298e-02,\n",
       "        -4.55182418e-02, -2.46079750e-02, -5.05300350e-02, -2.58913357e-03,\n",
       "        -3.00206691e-02,  4.16055147e-04, -6.80003837e-02, -3.85513194e-02,\n",
       "        -1.10312030e-02, -5.28457016e-02,  4.15741354e-02, -6.30180771e-03,\n",
       "        -5.85365901e-03, -1.93946976e-02,  2.64038257e-02,  8.87955446e-03,\n",
       "         3.83369089e-03,  3.21049541e-02, -8.43573064e-02,  1.75897181e-02,\n",
       "         3.38534154e-02,  3.42272110e-02,  1.14431428e-02,  1.76795740e-02,\n",
       "         1.00157633e-02, -1.10468185e-02,  9.13058072e-02, -2.03380361e-02],\n",
       "       dtype=float32),\n",
       " (768,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = [\n",
    "    \"The sky is blue today.\",\n",
    "    \"Machine learning uses mathematics to find patterns in data.\",\n",
    "    \"Neural networks are trained using backpropagation.\",\n",
    "    \"Backpropagation is the training method used to update neurons.\"\n",
    "]\n",
    "\n",
    "model.encode(strings[0]), model.encode(strings[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95752cc9-ab34-42be-8bdf-09777c814eb7",
   "metadata": {},
   "source": [
    "Hopefully you can see that the vector embedding looks very much like (even better, is identical to) the one in the solution. You can also see that each of these vectors is 768 dimensions, or $\\mathbb{R}^{768}$ space.\n",
    "\n",
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.4\n",
    "\n",
    "The model has a `similarity()` method associated with it:\n",
    "\n",
    ">```Help on function dot_score in module sentence_transformers.util:\n",
    "> dot_score(a: 'list | np.ndarray | Tensor', b: 'list | np.ndarray | Tensor') -> 'Tensor'\n",
    ">    Computes the dot-product dot_prod(a[i], b[j]) for all i and j.\n",
    ">    \n",
    ">    Args:\n",
    ">        a (Union[list, np.ndarray, Tensor]): The first tensor.\n",
    ">        b (Union[list, np.ndarray, Tensor]): The second tensor.\n",
    ">    \n",
    ">    Returns:\n",
    ">        Tensor: Matrix with res[i][j] = dot_prod(a[i], b[j])\n",
    "\n",
    "This function leverages the Linear Algebra dot product operation to determine a \"score\" for how similar two vectors are as a single value. The greater this value is, the more similar, or related, the two sentences are. A score close to zero implies the two vectors are not related. A negative value indicates that they are negatively related. In other words, they are talking about or \"moving\" in opposite directions.\n",
    "\n",
    "If you think about the phrases that we have embedded, the first sentence has nothing to do with the other three. The second and third, while not expressing the same idea, are definitely related. The last two are saying pretty much the same thing in different ways.\n",
    "\n",
    "Use the next cell to generate and print out the `similarity()` score for each pair of sentences. Which pair has the greatest similarity value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1ec634-6fc8-4b33-b193-0cff1cf2aecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0151]])\tThe sky is blue today., Machine learning uses mathematics to find patterns in data.\n",
      "tensor([[-0.0273]])\tThe sky is blue today., Neural networks are trained using backpropagation.\n",
      "tensor([[-0.0705]])\tThe sky is blue today., Backpropagation is the training method used to update neurons.\n",
      "tensor([[0.2563]])\tMachine learning uses mathematics to find patterns in data., Neural networks are trained using backpropagation.\n",
      "tensor([[0.1926]])\tMachine learning uses mathematics to find patterns in data., Backpropagation is the training method used to update neurons.\n",
      "tensor([[0.7777]])\tNeural networks are trained using backpropagation., Backpropagation is the training method used to update neurons.\n"
     ]
    }
   ],
   "source": [
    "def similarity(a, b):\n",
    "    print(f'{model.similarity(model.encode(a),model.encode(b))}\\t{a}, {b}')\n",
    "\n",
    "_ = [similarity(a,b) for idx, a in enumerate(strings) for b in strings[idx + 1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e21933-d31e-4b46-bb51-12130286653c",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.5\n",
    "\n",
    "That's pretty cool! You can see that the first sentence, while slightly negative in relation to the others, is very close to zero, or \"not related.\"  The second sentence has a stronger relationship with the last two, which is correct. Finally, the last two sentences, which are both saying the same thing in different ways have a very strong match at 0.777.\n",
    "\n",
    "To make use of this, then, we need to take some text (or corpus) of interest and convert it into chunks of text that we can then convert into embeddings. If we can accomplish this, we could store the embeddings somewhere and then leverage them to find related ideas in the corpus of text that we have processed. Let's do exactly this.\n",
    "\n",
    "There are many types of text data you may want to process. We're going to take a look at a few of the more common types of source data in this class. The first of these, and our focus in this lab, will be PDF files.\n",
    "\n",
    "To read in our PDF data, we are going to use the `PdfReader()` class from PyPDF. This class provides a very convenient interface for processing and distilling the text in a PDF file. The files that we are interested in loading are located in the relative path `'../data/source_docs/'`. Rather than pre-configuring a list of files to load, why don't we dynamically determine the names of the files in that directory so that we have the beginnings of a sort of \"pipeline\" to take source text to vector embeddings.\n",
    "\n",
    "Use the following cell to retrieve the list of files in the source directory. Store this list in a variable named `files`. You might use `list(Path('../data/source_docs/').glob('*.pdf'))` to obtain the list of PDF files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15296ed9-50b9-46f2-8b6f-6993a67ea82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/source_docs/NIST.SP.800-12r1.pdf'),\n",
       " PosixPath('../data/source_docs/Incident_Handling.pdf'),\n",
       " PosixPath('../data/source_docs/DEV543.pdf'),\n",
       " PosixPath('../data/source_docs/NIST.SP.800-53r5.pdf')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('../data/source_docs/')\n",
    "files = list(path.glob('*.pdf'))\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b4972-e765-480a-b2f2-541c8b308f1d",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.6\n",
    "\n",
    "Let's just focus on the *NIST SP 800-53* file for now.\n",
    "\n",
    "The `PdfReader()` class accepts a file path to a PDF file as an argument. It will read in and process this file, allowing us to interrogate it using the object that we create. \n",
    "\n",
    "Identify at which offset the \"NIST.SP.800-53r5.pdf\" file is found in the list of files. In the following cell, use `PdfReader()` to process this file and assign the output to a variable named `document`.\n",
    "\n",
    "**Note:** *Do not be concerned if you receive messages stating \"Ignoring wrong pointing object...\" if you choose to process some of the other files.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b59e3d-06d1-4995-8395-9f1de52b1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = PdfReader(files[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe25c72-1dc8-4551-8d65-a2dbcfe2724d",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.7\n",
    "\n",
    "The document is represented in memory as a series of page objects. Let's do two things:\n",
    "\n",
    " * Please print out the `len()` of `document.pages`. This will tell us how many pages there are.\n",
    " * Print out `str(document.pages[10])[:1000]` (i.e., the first 1,000 characters of the document when viewed as a string).\n",
    "\n",
    "There's nothing special about page 10; I just want us to skip somewhere into the body of the document rather than the title page to get a look at how the pages are represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef1998c-427d-4e18-9730-4e488a179470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document has 492 pages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'/Contents': IndirectObject(25, 0, 139822345706256), '/CropBox': [0.0, 0.0, 612, 792], '/Group': IndirectObject(2069, 0, 139822345706256), '/MediaBox': [0.0, 0.0, 612, 792], '/Parent': IndirectObject(60119, 0, 139822345706256), '/Resources': {'/ColorSpace': {'/CS0': IndirectObject(60270, 0, 139822345706256), '/CS1': IndirectObject(2558, 0, 139822345706256), '/CS2': IndirectObject(60271, 0, 139822345706256)}, '/ExtGState': {'/GS0': IndirectObject(2556, 0, 139822345706256)}, '/Font': {'/TT0': IndirectObject(60279, 0, 139822345706256), '/TT1': IndirectObject(2559, 0, 139822345706256), '/TT2': IndirectObject(60273, 0, 139822345706256)}, '/XObject': {'/Im0': IndirectObject(1465, 0, 139822345706256)}}, '/Rotate': 0, '/StructParents': 32, '/Tabs': '/S', '/Type': '/Page'}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'This document has {len(document.pages)} pages.')\n",
    "str(document.pages[10])[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7adf0-b516-495c-bbb8-3c4ffe090027",
   "metadata": {},
   "source": [
    "> ### On Document Types\n",
    "> Looking at the data you are about to process is a very important habit to get into. In this case, it should be very obvious that we cannot work with the page objects directly since they are just long strings of PostScript code (we will see how to recover the text from the pages momentarily). *Always* make sure your data looks the way you think it does.\n",
    ">\n",
    "> We have a great deal to cover in this class, so we will not have the time to experiment with different types of source documents. You should definitely plan to spend some time experimenting on different source document types on your own. We will be dealing with PDF files exclusively in this class, but even there we can run into problems. For example, consider the DEV543.pdf file that we will use. If you load that file and view the pages you will find that almost all of the whitespace in the file is composed of tab characters! In this specific case, our processes will still handle the file with no big issues, but could it be more ideal to replace those tabs with spaces? While this sounds like a simple solution, could it be that there are locations in the document where the tabs are intentional and important?\n",
    ">\n",
    "> Another file type that you will likely use a great deal are HTML files. While it is true that these files are just text files, it is usually a bad idea to try to import HTML files as-is. An exception would be if you are indending to do something with the generation of HTML code rather than textual content (which is our intent). In this case, using a library like BeautifulSoup to extract the text content for subsequent processing makes the most sense.\n",
    ">\n",
    "> There are even more obvious problems when we consider a source document, perhaps a PDF, that is composed of images of pages or text rather than text. In this case, a project like Tesseract can be quite useful for performing OCR at the page level across the entire document.\n",
    "\n",
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.8\n",
    "\n",
    "From the output above we can see that this document as 492 pages. The PostScript code used to build the page is also readable, though we cannot see any of the text that makes up the page. This is because the text content is encoded using \"Flate\" (deflate, actually) compression algorithm. Don't worry, we don't need to manually decode anything. The PdfReader interface provides an `extract_text()` method for each page.\n",
    "\n",
    "Use the next cell to output the result of `document.pages[10].extract_text()` and `document.pages[11].extract_text()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5e257d-1352-4e24-aa74-bde1a4222699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NIST  SP 800- 53, REV. 5                                                                                     SECURITY AND PRIVACY CONTROLS FOR INFORMATION SYSTEMS AND ORGANIZATIONS                                                                  \\n_________________________________________________________________________________________________  \\nix \\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 \\n   \\nINFORMATION SYSTEMS —  A BROAD -BASED PERSPECTIVE  \\nAs we push computers to “the edge ,” building an increasingly complex world of interconnected \\nsystems and devices, security and privacy continue to do minate the national dialogue. There is \\nan urgent need to further strengthen the underlying systems, products, and services that we \\ndepend on in every sector of the critical infrastructure to ensur e that  those systems, products , \\nand services are sufficiently trustworthy and provide the necessary resilience to support the \\neconomic and national security interests of th e United States. NIST Special Publication 800- 53, \\nRevision 5, responds to this need by embarking on a proactive and systemic approach to develop  \\nand make available to a broad base of public and private sector organizations a comprehensive \\nset of security and privacy safeguarding measures for all types of computing platforms, including \\ngeneral purpose computing systems, cyber -physical systems, clou d systems, mobile systems, \\nindustrial control systems,  and Internet of Things (IoT) devices.  Safeguarding measures include \\nboth security and privacy controls to protect the critical and essential operations and assets of \\norganizations and the privacy of individuals. The objective is to make the systems we depend on more penetration resistant to attacks , limit the damage from those attacks when they occur , \\nand make the systems resilient , survivable , and protective of individuals’ privacy.  \\n ',\n",
       " 'NIST  SP 800- 53, REV. 5                                                                                     SECURITY AND PRIVACY CONTROLS FOR INFORMATION SYSTEMS AND ORGANIZATIONS                                                                  \\n_________________________________________________________________________________________________  \\nx \\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 \\n  \\n  \\nCONTROL BASELINES   \\nThe control baselines that have previously been included in NIST Special Publication 800- 53 have \\nbeen relocated to NIST Special Publication 800 -53B. SP 800- 53B contains security and privacy \\ncontrol baselines for federal information systems and organizations. It provides guidance for  \\ntailoring control baselines and for developing o verlays to support the security and privacy \\nrequirements of stakeholder s and their organizations.  CNSS Instruction 1253  provides control \\nbaselines and guidance for security categorization and security control selection for national \\nsecurity systems.  \\n ')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.pages[10].extract_text(), document.pages[11].extract_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a66d0c-95f7-4dbc-abb6-8de522994854",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.9\n",
    "\n",
    "Look carefully at the output. Notice that the first thing output from each page is the string, \"NIST  SP 800- 53, REV. 5 ... SECURITY AND PRIVACY CONTROLS FOR INFORMATION SYSTEMS AND ORGANIZATIONS\". This appears to be the heading that appears as the first text element on every page in the PDF that has useful content (you can verify this for yourself if you open the PDF in a PDF viewer).\n",
    "\n",
    "In this case, could it make sense to split the extracted text on newline characters and then drop the first element in the resulting list? Let's try this out.\n",
    "\n",
    "In the following cell, iterate over the first 10 pages of the document, extracting the text. Split each page's text on newline characters, drop the first value (which will be everything left of the first newline) and print the resulting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9062df7-1521-4d23-a062-f4578db6ecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------> Page 1\n",
      "['Revision 5  ', ' ', ' ', 'Security and Privacy Control s for ', 'Information  Systems  and Organizations  ', ' ', ' ', ' ', ' ', 'JOINT TASK FORCE  ', '  ', ' ', ' ', 'This publication is available free of charge from:  ', 'https://doi.org/10.6028/NIST.SP.800- 53r5    ', '  ', ' ', '  ', ' ', '  ']\n",
      "---------> Page 2\n",
      "['Revision 5  ', ' ', ' ', ' Security and Privacy Controls for ', 'Information Systems and Organizations                                                                                                 ', ' ', '  ', 'JOINT TASK FORCE  ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'This publication is available free of charge from:  ', 'https://doi.org/10.6028/NIST.SP.800- 53r5   ', ' ', ' ', ' ', ' ', 'September  2020  ', 'INCLUDES UPDATES AS OF 12- 10-2020; SEE PAGE XVII  ', ' ', ' ', '  ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'U.S. Department of Commerce  ', 'Wilbur L. Ross, Jr., Secretary  ', ' ', 'National Institute of Standards and Technology  ', '      Walter Copan, NIST Director and Under Secretary of Commerce for Standards and Technology    ']\n",
      "---------> Page 3\n",
      "['_________________________________________________________________________________________________  ', 'i ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', ' Authority  ', 'This publication has been developed by NIST to further its statutory responsibilities under the ', 'Federal Information Security Modernization Act (FISMA), 44 U.S.C. § 3551 et seq. , Public Law ', '(P.L.) 113 -283. NIST is responsible for developing information security standards and guidelines, ', 'including minimum requirements for federal informatio n systems . Such information security ', 'standards and guidelines shall not apply to national security systems without the express ', 'approval of the appropriate federal officials exercising policy authority over such systems. This ', 'guideline is consistent with the requirements of the Office of Management and Budget (OMB) ', 'Circular A-130.  ', 'Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and binding on federal agencies by the Secretary of Commerce  under statutory ', 'authority. Nor should these guidelines be interpreted as altering or superseding the existing ', 'authorities of the Secretary of Commerce, OMB Director, or any other federal official. This ', 'publication may be used by nongovernmental organizati ons on a voluntary basis and is not ', 'subject to copyright in the United States. Attribution would, however, be appreciated by NIST.   ', 'National Institute of Standards and Technology Special Publication 800 -53, Revision 5  ', 'Natl. Inst. Stand. Technol.  Spec. Pub l. 800 -53, Rev. 5 , 492 pages  (September  2020 ) ', 'CODEN: NSPUE2  ', 'This publication is available free of charge from:  ', 'https://doi.org/10.6028/NIST.SP.800- 53r5   ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'Comments on this publication may be submitted to:  ', 'National Institute of Standards and Technology  ', 'Attn: Computer Security Division, Information Technology Laboratory  ', '100 Bureau Drive (Mail Stop 8930) Gaithersburg, MD 20899 -8930  ', 'Email: sec-cert@nist.gov   ', 'All comments are subject to release under the Freedom of Information Act (FOIA)  [FOIA96 ]. Certain commercial entities, equipment, or materials may be identified in this document to describe ', 'an experimental procedure or concept adequately. Such identification is not intended to imply ', 'recommendation or endorsement by NIST, nor is it intended to imply that the entities, materials, or ', 'equipment are necessarily the best available for the purpose.  ', 'There may be references in this publication to other publications currently under development by ', 'NIST in accordance with its assigned statutory responsibilities. The information in this publication, ', 'including concepts, practices, and methodologies may be u sed by federal agencies even before the ', 'completion of such companion publications. Thus, until each publication is completed, current ', 'requirements, guidelines, and procedures, where they exist, remain operative. For planning and ', 'transition purposes, federa l agencies may wish to closely follow the development of these new ', 'publications by NIST.   ', 'Organizations are encouraged to review draft publications during the designated public comment ', 'periods and provide feedback to NIST. Many NIST publications, other than the ones noted above, ', 'are available at https://csrc.nist.gov/publications .  ', ' ']\n",
      "---------> Page 4\n",
      "['_________________________________________________________________________________________________  ', 'ii ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', ' Reports on Computer Systems Technology  ', 'The National Institute of Standards and Technology (NIST) Information Technology Laboratory ', '(ITL) promotes the U.S. economy and public welfare by providing technical leadership for the ', 'Nation’s measurement and standards infrastructure. ITL develops tests, test methods, reference ', 'data, proof of concept implementations, and technical analyses to advance the development ', 'and productive use of informatio n technology (IT). ITL’s responsibilities include the development ', 'of management, administrative, technical, and physical standards and guidelines for the cost -', 'effective security of other than national security-related information in federal information systems. The Special Publication 800 -series reports on ITL’s research, guidelines, and outreach ', 'efforts in information systems security and privacy and its collaborative activities with industry, ', 'government, and academic organizations. ', 'Abstract  ', 'This publicati on provides a catalog of security and privacy c ontrols for information systems and ', 'organ izations to protect organizational operations and assets, individuals, other organizations, ', 'and the Nation from a diverse set of threats  and risks , including hostile attacks , human errors, ', 'natural disa sters, structural failures, foreign intelligence entities, and privacy risks . The controls ', 'are flexible and customizable and implemented as part of an organiz ation -wide process to ', 'manage risk. The contr ols address diverse requirement s derived from mission and business ', 'needs, laws , executive orders, directives, regulations, policies, standards , and guidelines . Finally, ', 'the consolidated control catalog address es security and privacy from a functionality perspective ', '(i.e., the strength of functions and mechanisms  provided by the controls ) and from an assurance ', 'perspective ( i.e., the measure of confidence in the security or privacy capability  provided by the ', 'controls ). Addressin g functionality and assurance helps to ensure that information technology  ', 'products and the systems that rely on those products are sufficiently trustworthy.  ', 'Keywords  ', 'Assurance; availability; computer security; confidentiality; control; cybersecurity; FISMA; ', 'information security; information system; integrity; personally identifiable information; Privacy ', 'Act; privacy controls; privacy functions; privacy requirements; Risk Management Framework; ', 'security controls; security function s; security requirements ; system; system security .  ']\n",
      "---------> Page 5\n",
      "['_________________________________________________________________________________________________  ', 'iii ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', ' Acknowledgements  ', 'This publication was developed by the Joint Task Force  Interagency Working Group. The group ', 'includes representatives from the c ivil, defense, and intelligence communities. The National ', 'Institute of Standards and Technology wishes to acknowledge and thank the senior leaders from ', 'the Department of Commerce , Dep artment of Defense, the Office of the Director of National ', 'Intelligence, the Committee on National Security Systems, and the members of the interagency working group whose dedicated efforts contributed significantly to th is publication. ', 'Department of Defen se Office of the Director of National ', 'Intelligence  ', 'Dana Deasy       Matthew A. Kozma                                                                                                                                                                                                 ', 'Chief Information Officer      Chief Information Officer  ', 'John Sherman                                                                                   Michael E. Waschull  ', 'Principal Deputy CIO  Deputy Chief Information Officer  ', 'Mark Hakun                                                                                    Clifford M. Conner  ', 'Deputy CIO for Cybersecurity  and DoD SISO  Cybersecurity Group and IC CISO  ', 'Kevin Dulany       Vacant  ', 'Director, Cybersecurity Policy and Partnerships    Director, Security Coordination Center ', 'National Institute of Standards    Committee on National Security ', 'and Technology      Systems  ', 'Charles H. Romine       Mark G. Hakun  ', 'Director, Information Technology Laboratory  Chair  ', 'Kevin Stine       Susan Dorr  ', 'Acting Cybersecurity Advisor, ITL     Co-Chair  ', 'Matthew  Scholl       Kevin Dulany  ', 'Chief, Computer Security Division     Tri-Chair— Defense Community  ', 'Kevin Stine       Chris Johnson  ', 'Chief, Applied Cybersecurity Division     Tri-Chair— Intelligence Community  ', 'Ron Ross        Vicki Michetti  ', 'FISMA Implementation Project Leader     Tri-Chair— Civil Agencies  ', 'Joint Task Force Working Group  ', 'Victoria Pillitteri        McKay Tolboe    Dorian Pappas   Kelley Dempsey  ', 'NIST, JTF Leader        DoD    Intelligence Community  NIST  ', 'Ehijele Olumese        Lydia Humphries    Daniel Faigin   Naomi Lefkovitz  ', 'The MITRE Corporation        Booz Allen Hamilton   Aerospace Corporation  NIST  ', 'Esten Porter       Julie Nethery Snyder   Christina Sames   Christian Enloe  ', 'The MITRE Corporation       The MITRE Corporation   The MITRE Corporation  NIST  ', 'David Black        Rich Graubart    Peter Duspiva   Kaitlin Boeckl   ', 'The MITRE Corporation       The MITRE Corporation   Intelligence Community  NIST     ', 'Eduardo Takamura       Ned Goren    Andrew Regenscheid Jon Boyens   ', 'NIST         NIST     NIST    NIST  ']\n",
      "---------> Page 6\n",
      "['_________________________________________________________________________________________________  ', 'iv ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', ' In addition to the above acknowledgments, a special not e of thanks goes to Jeff Brewer, Jim ', 'Foti, and the NIST web team for their outstanding administrative support. The authors also wish ', 'to recognize Kristen Baldwin, Carol Bales, John Bazile, Jennifer Besceglie, Sean Br ooks, Ruth ', 'Cannatti, Kathleen Coupe, Keesha Crosby, Charles Cutshall, Ja’Nelle DeVore, Jennifer Fabius, Jim ', 'Fenton, Hildy Ferraiolo, Ryan Galluzzo, Robin Gandhi, Mike Garcia, Paul Grassi, Marc Groman , ', 'Matthew Halstead, Kevin Herms, Scott Hill, Ralph Jones, Martin Kihiko, Raquel Leone, Jason ', 'Marsico, Kirsten Moncada, Ellen Nadeau, Elaine Newton, Michael Nieles, Michael Nussdorfer, ', 'Taylor Roberts, Jasmeet Seehra, Joe Stuntz, Jeff Williams, the professional staff  from the NIST ', 'Computer Security  Division and Applied Cybersecurity Division , and the representatives from ', 'the Federal CIO Council , Federal CISO Council, Federal Privacy Council, Control Baseline ', 'Interagency Working Group , Security and Privacy Collaboration Working Group , and  Federal ', 'Privacy Council  Risk Management Subcommittee for their ongoing contributions in helping to ', 'improve th e content of the publication.  Finally, the authors gratefully acknowledge the ', 'contributions fr om individuals and organizations in the public and private sectors,  both ', 'nationally and internationally, whose insightful  and constructive comments improved the ', 'overall quality, thoroughness, and usefulness of this publication.   ', 'HISTORICAL CONTRIBUTIONS TO NIST SPECIAL PUBLICATION 800-53 ', 'The authors wanted to acknowledge the many individuals who contributed to previous versions ', 'of Special Publication 800 -53 since its inception in 2005. They include Marshall Abrams, Dennis ', 'Bailey, Lee Badger, Curt Barker, Matt hew  Barrett, Nadya Bartol, Frank Belz, Paul Bicknell, Deb ', 'Bodeau, Paul Brusil, Brett Burley, Bill Burr, Dawn Cappelli, Roger Caslow, C orinne Castanza, Mike ', 'Cooper, Matt Coose, Dominic Cussatt, George Dinolt, Randy Easter, Kurt Eleam,  Denise Farrar, ', 'Dave Ferraiolo, Cita Furlani, Harriett Goldman, Peter Gouldmann, Tim Grance, Jennifer Guild, Gary Guissanie, Sarbari Gupta, Priscilla Guthrie , Richard Hale, Peggy Himes, Bennett Hodge, ', 'William Hunteman, Cynthia Irvine, Arnold Johnson, Roger Johnson, Don ald Jones, Lisa Kaiser, ', 'Stuart Katzke, Sharon Keller, Tom Kellerman n, Cass Kelly, Eustace King, Daniel Klemm, Steve ', 'LaFountain, Annabelle Lee, R obert Lentz, Steven  Lipner, William  MacGregor, T homas  Macklin, ', 'Thomas  Madden, Robert Martin, Erika McCallister, Tim McChesney, Michael McEvilley, Rosalie ', 'McQuaid, Peter Mell, John Mildner, Pam Miller, Sandra Miravalle, Joji Montelibano, Doug las ', 'Montgomery,  George Moore, Rama Moorthy, Mark Morrison, Harvey Newstrom, Sherrill Nicely, ', 'Robert Niemeyer, LouAnna Notargiacomo, Pat O’Reilly, Tim Polk, Karen Quigg, Steve Quinn, ', 'Mark Riddle, Ed Roback, Cheryl Roby, George Rogers, Scott Rose, Mike Rubin, Karen Scarfone, ', 'Roger Schell, Jackie Snouffer, Ray Snouffer, Murugiah Souppaya, Gary Stoneburner, Keith ', 'Stouffer, Marianne Swanson, Pat Toth, Glenda Turner, Pat rick Viscuso, Joe Weiss, Richard ', 'Wilsher, Mark Wilson, John Woodward, and Carol Woody.  ', ' ', ' ']\n",
      "---------> Page 7\n",
      "['_________________________________________________________________________________________________  ', 'v ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', ' Patent Disclosure Notice  ', 'NOTICE: The Information Technology Laboratory (ITL) has requested that holders of patent ', 'claims whose use may be required for compliance with the guidance or requirements of this ', 'publication disclose such patent claims to ITL. However, holders of patents are not obligated to ', 'respond to ITL calls for patents and ITL has not undertaken a patent search in order to identify ', 'which, if any, patents may apply to this publication.  ', '  ', 'As of the date of publication and following call(s) for the identification of patent claims whose use may be required for compliance with the guidance or requirements of this publication, no such patent claims have been identified to ITL.   ', '  ', 'No representation is made or implied by ITL that licenses are not required to avoid patent ', 'infringement in the use of this publication.   ']\n",
      "---------> Page 8\n",
      "['_________________________________________________________________________________________________  ', 'vi ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', '   ', 'RISK MANAGEMENT  ', 'Organizations must exercise  due diligence in managing information security  and privacy risk. This ', 'is accomplished, in part, by  establish ing a comprehensive risk management program  that uses ', 'the flexibility inherent in NIST publications to categorize systems, select and implement security ', 'and priva cy controls that meet mission and business needs, assess the effectiveness of the ', 'controls, authorize the systems for operation, and continuously monitor the system s. Exercising ', 'due diligence and implementing robust and comprehensive information security a nd privacy risk ', 'management programs can facilitate compliance with applicable laws, regulations, executive ', 'orders, and governmentwide policies. Risk management frameworks and risk management ', 'processes are essential in developing, implementing, and maintain ing the protection measures ', 'necessary to address stakeholder needs and the current threats to organizational operations and assets, individuals, other organizations, and the Nation. Employing effective risk -based ', 'processes, procedures, methods, and technol ogies ensure s that information systems and ', 'organizations have the necessary trustworthiness and resiliency to su pport essential mission and ', 'business functions, the U.S. critical infrastructure , and continuity of government.  ', ' ']\n",
      "---------> Page 9\n",
      "['_________________________________________________________________________________________________  ', 'vii ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', '   ', 'COMMON SECURITY AND PRIVACY FOUNDATIONS  ', 'In working with the Office of Management and Budget to develop standards and guidelines ', 'required by FISMA, NIST consults with federal agencies, state, local, and tribal governments, and ', 'private sector organizations to improve information security and privacy , avoid unnecessary and ', 'costly duplication of effort , and help ensure that its publications are complementary with the ', 'standards and guidelines used for the protection of national security systems. In addition to a ', 'comprehensive and t ransparent public review and comment process, NIST is engaged in a ', 'collaborative partnership with the Office of Management and Budget, Office of the Director  of ', 'National Intelligence, Department of Defense, Committee on N ational Security Systems, Federal ', 'CIO Council, and Federal Privacy Council to establish  a Risk Management Framework  (RMF)  for ', 'information security and privacy  for the Federal Government. This co mmon foundation provides ', 'the Federal Government and their cont ractors  with cost-effective, flexible, and consi stent ways ', 'to manage  security and privacy  risks to organizational operations and assets, individuals, other ', 'organizatio ns, and the Nation. The framework provide s a basis for the reciprocal acceptance of ', 'secur ity and privacy control assessment evidence and authorization decisions and facilitates ', 'information sharing and collaboration. NIST continues to work with public and private sector ', 'entities to establish mappings and relationships between the standards and guidelines ', 'developed by NIST and those developed by other organizations.  NIST anticipates using these ', 'mappings and the gaps they identify to improve the control catalog.  ', ' ', ' ']\n",
      "---------> Page 10\n",
      "['_________________________________________________________________________________________________  ', 'viii ', 'This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.800 -53r5 ', '   ', 'DEVELOPMENT OF INFORMATION SYSTEMS, COMPONENTS, AND SERVICES  ', 'With a  renewed emph asis on the use of trustworthy, secure information systems and supply ', 'chain security, it is essential that organizations express their security and privacy requirements ', 'with clarity and specificity in order to  obtain the systems, components, a nd services necessary ', 'for mission and business success. Accordingly, this publica tion provides controls in the System ', 'and Services Acquisition (SA) and Supply Chain Risk Management (SR) families that are directed ', 'at developers . The scope of the controls in those families includes information system, system ', 'component, and system service development and  the associated developers whether the ', 'development i s conducted internally by organizations or externally through the contracting and ', 'acquisition processes. Th e affected controls in the control catalog include SA-8, SA-10, SA-11, ', 'SA-15, SA-16, SA-17, SA-20, SA-21, SR-3, SR-4, SR-5, SR-6, SR-7, SR-8, SR-9, and SR-11. ']\n"
     ]
    }
   ],
   "source": [
    "for page_number, page in enumerate(document.pages[:10]):\n",
    "    text = page.extract_text().split('\\n')[1:]\n",
    "    print(f'---------> Page {page_number + 1}')\n",
    "    print(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1236969-a875-4145-b8b8-ff9f21ef615e",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.10\n",
    "\n",
    "That looks pretty good, but it's not perfect. It appears that the first few pages are just front matter in the document, but when the content starts it appears that each page starts with a long horizontal bar (underscore characters). Let's take what we did in the last cell, adjust it to skip the header and the underscores, and turn that into a function.\n",
    "\n",
    "Use the following cell to define a function named `get_text()` that will accept a page from a document, extract the text, split on newlines, and eliminate the heading and the underscores (the first two values in the resulting list), returning the remainder of the text. Test your function on a page in the document. Continue to adjust the number of lines that you skip so that extraneous page content is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18951bd-fabf-40f2-8f98-2888bde24df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mechanisms can  provide confidentiality and integrity protections depending on the mechanisms \\nimplemented . Activities associated with media transport include releasing media for transport, \\nensuring that media enters the appropriate transport proces ses, and  the actual tran sport . \\nAuthorized transport and courier personnel may i nclude individuals external to the organization.  \\nMaintaining accountability of media during transport includes  restricting transport activities to \\nauthorized personnel and tracking and/or obtaining records of transport activities as the media \\nmoves through the transportation system to prevent and detect loss, destruction, or tampering. \\nOrganizations establish documentation requirements for activities associated with the transport \\nof system media in acco rdance with organizational assessments of risk . Organizations maintain \\nthe flexibility to define record -keeping methods for the different types of media transport as part \\nof a system of transport- related records.  \\nRelated Controls :  AC-7, AC-19, CP-2, CP-9, MP-3, MP-4, PE-16, PL-2, SC-12, SC-13, SC-28, SC-34. \\nControl Enhancements : \\n(1) MEDIA TRANSPORT | PROTECTION OUTSIDE OF CONTROLLED AREAS  \\n[Withdrawn: Incorporated into MP-5.] \\n(2) MEDIA TRANSPORT | DOCUMENTATION OF ACTIVITIES  \\n[Withdrawn: Incorporated into MP-5.] \\n(3) MEDIA TRANSPORT | CUSTODIANS   \\nEmploy an identified custodian during transport of system  media outside of controlled \\nareas.  \\nDiscussion :  Identified custodians provide organizations with specific points of contact during \\nthe media transport process and facilitate individual accountability. Custodial responsibilities \\ncan be transferred from one individual to another if  an unambiguous custodian is identified.  \\nRelated Controls :  None.  \\n(4) MEDIA TRANSPORT | CRYPTOGRAPHIC PROTECTION  \\n[Withdrawn: Incorporated into SC-28(1) .] \\nReferences :  [FIPS 199], [SP 800- 60-1], [SP 800- 60-2]. \\nMP-6 MEDIA SANITIZATION  \\nControl : \\na. Sanitize [ Assignment: organization- defined system media ] prior to disposal, release out of \\norganizational control, or release for reuse using [ Assignment: organization- defined \\nsanitization techniques and procedures ]; and  \\nb. Employ sanitization mechanisms with the strength and integrity commensurate with the security category or classification of th e information.  \\nDiscussion :   Media sanitization applies to all digital and non -digital system media subject to \\ndisposal or reuse, whether or not the media is considered removable. Examples include digital \\nmedia in scanners, copiers, printers, notebook computers, workstations, network components, \\nmobile devices , and non -digital media (e.g., paper and microfilm ). The sanitization process \\nremoves information from system media such that the information cannot be retrieved or \\nreconstructed. Sanitiza tion techniques —including clearing, purging, cryptographic erase, de -\\nidentification of personally identifiable information,  and destruction —prevent the disclosure of \\ninformation to unauthorized individuals when such media is reused or released for disposal. \\nOrganizations determine the appropriate sanitization methods , recognizing that destruction is \\nsometimes necessary when other methods cannot be applied to media requiring sanitization. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text(page, lines_to_skip = 4):\n",
    "    \"\"\"\n",
    "    Here's the logic of the one liner below:\n",
    "        Extract the text (page.extract_text())\n",
    "        Split the result on newlines (.split('\\n'))\n",
    "        Ignore the element at position 0 ([1:])\n",
    "        Join that list with newlines to create a single string ('\\n'.join())\n",
    "            Note that we are preserving all of the original newlines since they\n",
    "            should tell us where paragraphs are. Semantically, we expect\n",
    "            all of the sentences in a paragraph to be somewhat related\n",
    "            and a new paragraph to indicate a change in thought.\n",
    "    \"\"\"\n",
    "    return '\\n'.join(page.extract_text().split('\\n')[lines_to_skip:])\n",
    "\n",
    "get_text(document.pages[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88846b16-997b-4b90-8532-f55f38769862",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.11\n",
    "\n",
    "We could turn this text into embeddings as-is. It is certainly worth experimenting with using an entire page worth of content to generate the sentence embeddings, but my experience tells me this isn't going to be ideal. Another fairly common approach is to embed each paragraph. While this is better, in my experience, than embedding an entire page, we can generally do even better.\n",
    "\n",
    "While there is no \"right way\" to chunk the text, think about what the goal of your project is. In this case, we would like to ask questions related to the material and get answers based on the material. If the embeddings are too general (page level embeddings), it is unlikely that any pages match a question very well, but many pages will have mediocre matches because they are tangentially connected. On the other hand, if we make the embeddings too granular, perhaps every five words on the page, we are now hyper-localized and will be unlikely to find general concepts or thoughts. In the end, you need to experiment with the particular documents and type of data those documents contain to find the best fit for your application.\n",
    "\n",
    "Instead, we're going to try to chunk things first as paragraphs, then as sentences, and finally as words within one sentence. To do this, we are going to use the `RecursiveCharacterTextSplitter()`, which provides various options for customizing how the text is broken up. Its default is exactly the approach we are looking for.\n",
    "\n",
    "Instead of trying to describe it more, let's see what this approach does to a page of our text. In the following cell, define the following text splitter object and then use that to split the result of our `get_text()` function using `document.pages[50]` as the input.\n",
    "\n",
    "```\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=25,\n",
    "    length_function=len\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4048524-e280-49eb-bbd7-6f804ad5b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enforce [ Assignment: organization- defined mandatory access control policy ] over the set',\n",
       " 'of covered subjects and objects specified in the policy,  and where the policy:',\n",
       " '(a) Is uniformly enforced across the covered subjects and objects within the system;',\n",
       " '(b) Specifies that a subject that has been granted access to information is constrained',\n",
       " 'from doing any of the following;',\n",
       " '(1) Passing the information to unauthorized subjects or objects;',\n",
       " '(2) Granting its privileges to other subjects;',\n",
       " '(3) Changing one or more security attributes ( specified  by the policy) on subjects,',\n",
       " 'objects, the system, or system components;',\n",
       " '(4) Choosing the security attributes and attribute values ( specified by the policy) to',\n",
       " 'be ass ociated with newly created or modified objects; and',\n",
       " '(5) Changing the rules governing access control; and',\n",
       " '(c) Specifies that [ Assignment: organization- defined subjects ] may explicitly be granted',\n",
       " '[Assignment: organization- defined privileges ] such that they are not l imited by any',\n",
       " 'defined subset (or all) of the above constraints.',\n",
       " 'Discussion :  Mandatory access co ntrol is a type of nondiscretionary access  control .',\n",
       " 'Mandatory access control policies constrain what actions subjects can take with information',\n",
       " 'obtained from objects for which they have already been granted access. This prevents the',\n",
       " 'subjects from passing the information to unauthorized subjects and objects. Mandatory',\n",
       " 'access control policies constrain actions that subjects can take with respect to the',\n",
       " 'propagation of access control privileges; that is, a subject with a privilege cannot pass that',\n",
       " 'privilege to other subjects. The policy is uniformly enforced over  all subjects and objects to',\n",
       " 'which the system has control . Otherwise, the access control policy can be circumvented. This',\n",
       " 'enforcement is provided by an implementation that meets the reference monitor concept as',\n",
       " 'described in AC-25. The policy is bounded by the system (i.e., once the information is passed',\n",
       " 'outside of the control of the system, additional means may be required to ensure that the',\n",
       " 'constraints on the information remain in effect).',\n",
       " 'The trusted subjects described above are granted privileges consistent with the concept of',\n",
       " 'least privilege (see AC-6). Trusted subjects are only given the minimum privileges necessary',\n",
       " 'for satisfying organizational mission/business needs relative to the above policy . The control',\n",
       " 'is most applicable when there is a mandate that establishes a policy regarding access to',\n",
       " 'controlled unclassified information or  classified information and some users of the system',\n",
       " 'are not authorized access to all such information resident in the system. Mandatory access',\n",
       " 'control can operate in conjunction with  discretionary access control  as described in  AC-3(4).',\n",
       " 'A subject constrained in its operation by mandatory access control policies can still operate',\n",
       " 'under the les s rigorous constraints of AC- 3(4), but mandatory access control policies take',\n",
       " 'precedence over the less rigorous constraints of AC- 3(4). For example, while a mandatory',\n",
       " 'access control policy imposes a constraint that prevent s a subject from passing informati on',\n",
       " 'to another subject operating at a different impact or classification  level , AC-3(4) permits the',\n",
       " 'subject to pass the information to any other subject with the same impact or classification',\n",
       " 'level  as the subject.  Examples of mandatory access control policies include the Bell -LaPadula',\n",
       " 'policy to protect confidentiality of information and the Biba policy to protect the integrity of',\n",
       " 'information.  \\nRelated Controls :  SC-7. \\n(4) ACCESS ENFORCEMENT | DISCRETIONARY ACCESS CONTROL',\n",
       " 'Enforce [Assignment: organization- defined discretionary access control policy ] over the set',\n",
       " 'of covered subjects and objects specified in the policy, and where the policy specifies that a',\n",
       " 'policy specifies that a subject that has been granted access to information can do one or more of',\n",
       " 'can do one or more of the following:',\n",
       " '(a) Pass the information to any other subjects or objects;']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=25,\n",
    "    length_function=len\n",
    ")\n",
    "splitter.split_text(get_text(document.pages[50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5950ce7-d59f-449e-b91c-c680032ab49d",
   "metadata": {},
   "source": [
    "Take a few moments to look at the lines of text that are output and the configuration that we set up. Let's consider each option:\n",
    "\n",
    " * `chunk_size = 100` - This keyword argument allows us to control the maximum length (in characters) of any text string output by the text splitter. 100 is likely a bit small. It is worth experimenting with different chunk sizes to find a value that seems ideal for the specific text you are processing.\n",
    " * `chunk_overlap = 25` - This argument controls how many characters (maximum) each string will overlap the preceding and following strings. This allows us to tune how much information from each neighboring chunk is included. While information will cross over and appear in both chunks, our intention is to prevent an important idea from being cut in half, making it very difficult to find in the data later.\n",
    " * `length_function = len` - This argument allows us to pass in the name of a function that we wish to use to determine the length of a chunk. By using the builtin `len()` function, we are determining the length based on the number of characters. We could write our own length function to define length in any way that we wish to (perhaps the number of words).\n",
    "\n",
    "We do not yet know whether the length of the chunks works well for this set of data. We will conduct some experiments and determine a number that seems to work well.\n",
    "\n",
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.12\n",
    "\n",
    "Use the cell below to create a `get_chunks()` function. This function should use the `splitter` to split text into chunks and return the list of chunks. Verify your function works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae7c3c2-c3e2-46aa-a524-1fd4bca22a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['potential loss of confidentiality of the information asset does not affect privacy, security',\n",
       " 'objectives are the primary driver for the selection of the control. However, the implementation',\n",
       " 'of the control with respect to monitoring for unauthorized access could involve the processing',\n",
       " 'of PII which may result in privacy risks and affect privacy program objectives. The discussio n',\n",
       " 'section in AU -3 includes privacy risk considerations so that organizations can take those',\n",
       " 'considerations into account as they determine the best way to implement the control.',\n",
       " 'Additionally, the control enhancement AU-3(3)  (Limit Personally Identifiable Information',\n",
       " 'Elements) could be selected to support managing these privacy risks.',\n",
       " 'Due to permutations in the relationship between information security and privacy program objectives',\n",
       " 'program objectives  and risk management , there is a need for close collaboration between programs',\n",
       " 'between programs to',\n",
       " 'select and implement the appropriate controls for information systems processing PII.',\n",
       " 'Organizations consider how to promote and institutionalize collaboration between the two',\n",
       " 'programs to ensure that the objectives of both disciplines are met  and risks are appropriately',\n",
       " 'managed .\\n29 \\n2.5   TRUSTWORTHINESS  AND  ASSURANCE',\n",
       " 'The trustworthiness of systems, system co mponents, and system services  is an important part',\n",
       " 'of the risk management strategies developed by organizations.30 Trustworthiness, in this',\n",
       " 'context, means worthy of being trust ed to fulfill whatever requirements  may be needed for a',\n",
       " 'component, subsystem, system, network, application, mission, business function, enterprise, or',\n",
       " 'other entity.31 Trustworthiness requirements can include attributes of reliability, dependability,',\n",
       " 'performance, resilience, safety, security, privacy, and survivab ility under a range of potential',\n",
       " 'adversity in the form of disruptions, hazards, threats , and privacy  risks . Effective measures of',\n",
       " '. Effective measures of',\n",
       " 'trustworthiness are mea ningful only to the extent that the requirements are complete , well-',\n",
       " 'defined,  and can be accurately assessed.',\n",
       " 'Two fundamental concepts  that affect the trustworthiness of systems are functionality  and',\n",
       " 'assurance . Functionality is defined in terms of the security and privacy features, functions,',\n",
       " 'mechanisms, services, procedures, and architectures implemented within organizational',\n",
       " 'systems and programs and the environments in which those systems and programs operate.',\n",
       " 'Assurance is the measure of confidence that the system functionality is implemented correctly,',\n",
       " 'operating as intended, and producing the desired outcome with respect to meeting the secur ity',\n",
       " 'and privacy requirements for the system— thus possessing the capability to accurately mediate',\n",
       " 'and enforce established security and privacy policies.',\n",
       " 'In general, the task of prov iding meaningful assurance that a system is likely to do what is',\n",
       " 'expect ed of  it can be enhanced by techniques that simplify or narrow t he analysis by, for',\n",
       " 'example, increasing the discipline applied to the system architecture, software design,',\n",
       " 'specifications, code style , and configuration management. Security and privacy controls a ddress',\n",
       " 'functionality and assurance. Certain  controls focus primarily on functionality  while o ther',\n",
       " 'controls focus primarily on assurance. Some controls can support functionality and assurance.',\n",
       " '29 Resources to support information security and privacy program collaboration are available at',\n",
       " 'are available at  [SP 800- 53 RES ].',\n",
       " '30 [SP 800- 160- 1] provides guidance on systems security engineering and the application of',\n",
       " 'and the application of security design principles',\n",
       " 'to achieve trustworthy systems.  \\n31 See [ NEUM04 ].']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_chunks(page, splitter):\n",
    "    return splitter.split_text(get_text(page))\n",
    "\n",
    "get_chunks(document.pages[40], splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8b63e-611a-4854-aa18-8d97217dfe8f",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.13\n",
    "\n",
    "Let's take stock. Here's what we have so far:\n",
    "\n",
    " * A model that can translate text into embedding vectors.\n",
    " * An input PDF document, split into page objects.\n",
    " * A function that can take a page of text and return that page as overlapping chunks.\n",
    "\n",
    "Here's what we still need to do:\n",
    "\n",
    " * Convert our chunks of text into embeddings.\n",
    " * Store the embeddings along with the original text into the vector database.\n",
    " * Perform a search in the vector database for things related to a question.\n",
    " * Feed the question, the search results, and a prompt to our LLM to generate a response.\n",
    "\n",
    "It may seem as though there is more left to do than we have accomplished already, but what remains will go quite quickly. Let's start with the first outstanding bullet.\n",
    "\n",
    "In the following cell, write a function that will accept a page object and return a list of tuples. In each tuple, the original text should be the first element and the embedding vector should be the second element.\n",
    "\n",
    "**Note:** *If you are working on an ARM based Mac, you may find the performance to be slow. This has to do with how the container is virtualized on the Mac. If you install all of the prerequisite libraries (mentioned in the lab setup instructions) natively and run these notebooks directly on your host you will find the performance is more than acceptible. If you wish to go this route, you can continue to run the Milvus and Ollama containers but running the Jupyter notebooks from your host OS, which will allow the model to run natively as ARM code and allow the Mac GPU to be leveraged.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93dc9a4d-7093-4510-8cae-8cfd1bf6f182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Withdrawn: Moved to SC -45(2) .] \n",
      "References :  None . \n",
      "AU-9 PROTECTION OF AUDIT INFORMATION\n",
      "[ 0.00897559  0.03029372 -0.00777875  0.05310851  0.0243885 ]\n",
      "Control :\n",
      "[-0.0787594   0.00766547 -0.00090127  0.04242965  0.02315489]\n",
      "a. Protect audit information and audit logging tools from unauthorized access, modification,\n",
      "[-0.00447991  0.06485244 -0.04550175  0.05918524  0.09146447]\n",
      "and deletion ; and\n",
      "[-0.02405787  0.06054065 -0.03553287  0.01448782  0.06969618]\n",
      "b. Alert [ Assignment: organization- defined personnel or roles ] upon detection of unauthorized\n",
      "[ 0.02074453 -0.00076108 -0.02485767  0.02495249  0.02422264]\n",
      "access, modification, or deletion of audit information.\n",
      "[-0.04841081  0.0552009  -0.03602372  0.04804078  0.07497172]\n",
      "Discussion :  Audit information includes all information  needed to successfully audit system\n",
      "[-0.04029271  0.07498271  0.00583133  0.10176983  0.07268541]\n",
      "activity , such as  audit records, audit log settings, audit reports, and personally identifiable\n",
      "[0.00466998 0.04914423 0.00291541 0.04168045 0.09795113]\n",
      "information . Audit logging tools are those programs and devices used to conduct system audit\n",
      "[-0.05653402  0.07018787 -0.02304581  0.07152885  0.06716828]\n",
      "and logging activities. Protection of audit information focuses on technical protection and limits\n",
      "[-0.00459277  0.05458799 -0.04556353  0.05459367  0.08802938]\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(page, model, splitter):\n",
    "    results = []\n",
    "    chunks = get_chunks(page, splitter)\n",
    "    for chunk in chunks:\n",
    "        results.append((chunk, model.encode(chunk)))\n",
    "    return results\n",
    "\n",
    "for (text, embedding) in get_embeddings(document.pages[100], model, splitter)[:10]: #Limit to the first 10 to keep the output short\n",
    "    print(text)\n",
    "    print(embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e029f-f7e3-4930-a2a7-82988abd3015",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.14\n",
    "\n",
    "Now that we are generating embeddings we need to get them stored in the vector database. For our labs, we are using Milvus, but please understand that any vector database will work in mostly the same way. Our goal is to create a vector store that can hold the source text chunks and the related vector for every page in the document.\n",
    "\n",
    "In the following cell, we have already written the code that will establish the connection to our Milvus container. We aren't going to dig very deeply into the Milvus API and related code since it is really only presented as an example; we needed to talk to some vector database and Milvus was convenient. Ultimately, you will need to research the API for any vector store that you decide to use for your own projects. The code presented can act as a starting point should you decide to try out Milvus for production work.\n",
    "\n",
    "Since we are leveraging containers, it's pretty simple to identify the host by name (`milvus-standalone`) rather than needing to work out what the dynamic IP address is. The Milvus service operates on port 19530 by default.\n",
    "\n",
    "After connecting to the server, we are checking to see if the `SEC495` database already exists. If it does, we attach to that database. If it doesn't, we create the database.\n",
    "\n",
    "Notice that there are several lines of code line commented out. These lines can be uncommented if you wish to delete the database and start over fresh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a146091-ca05-4735-98e2-b5e5ac2fe11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEC495 already exists\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(\"http://milvus-standalone:19530\")\n",
    "\n",
    "# Uncomment the following lines if you wish to delete the SEC495 database in order to start over.\n",
    "#client.using_database('SEC495')\n",
    "#client.drop_collection('Lab_3')\n",
    "#client.using_database('default')\n",
    "#client.drop_database(\"SEC495\")\n",
    "\n",
    "if \"SEC495\" in client.list_databases():\n",
    "    print(\"SEC495 already exists\")\n",
    "    client.using_database(\"SEC495\")\n",
    "else:\n",
    "    print(\"Creating SEC495\")\n",
    "    client.create_database(\"SEC495\")\n",
    "    client.using_database(\"SEC495\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddc256-68dc-4504-a462-bffdafd5ca78",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.15\n",
    "\n",
    "Now that we have a connection to the database that we want to use, we need to either connect to an existing collection (like a table in a SQL database) or create a new collection. This is code that we want to think about a bit more deeply.\n",
    "\n",
    "When creating the collection, we have the opportunity to define the schema (design, structure) for that collection. This includes not only the size of the vectors that we will be storing but also any other \"metadata,\" or other information, that we wish to store in the collection. For now we are going to use the Milvus API defaults, with the exception of the `auto_id` feature. If we do not set this kwarg to `True`, we will be responsible for generating IDs for our vectors. There is no good reason for us to be concerned with generating these IDs in our case.\n",
    "\n",
    "Take a moment to consider the code in the following cell. When you are confident you have a feel for what it does, please execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37dbf30c-65cb-4fae-ab4c-7844f5f40ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.has_collection(collection_name=\"Lab_3\"):\n",
    "    client.drop_collection(collection_name=\"Lab_3\")\n",
    "client.create_collection(\n",
    "    collection_name=\"Lab_3\",\n",
    "    dimension=768,\n",
    "    auto_id=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7551f-fae7-4a52-8d05-cc0221360d86",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.16\n",
    "\n",
    "With the database and collection created, we can begin storing our vectors. While it is possible to perform these inserts one at a time, this approach is not particularly efficient. It is much better to insert *batches*, or groups, of records. To understand how to structure these, let's talk about how the `insert()` call works.\n",
    "\n",
    "The client object that we have connected to the database has an `insert()` method. We can use this method to insert individual entries or batches of entries. There are two kwargs of particular interest for us:\n",
    "\n",
    " * `collection_name` must be passed, indicating the collection into which the data should be inserted.\n",
    " * `data` must be passed, providing the data that we wish to insert.\n",
    "\n",
    "The `data` argument allows us to pass a list of objects to be inserted to the database driver. Each object in the list should be a dictionary with the following two keys:\n",
    "\n",
    " * `vector`: The value of this key must be a vector matching the size configured when the collection was created (768, in our case).\n",
    " * `text`: The value of this key will be the source text used to generate the associated vector.\n",
    "\n",
    "Using the following cell, write a function that accepts a document, the embedding model, and the text splitter. The function should iterate over all of the pages in the document, building a list of dictionary objects (as described above) for each page. Use the `client.insert()` function (as described above) to insert the list of objects from each page.\n",
    "\n",
    "**Note:** *You may wish to include some sort of status in your iteration loop since there are more than 490 pages and this task will take some time to complete.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c77d1b0-5f67-4fd4-915e-677b79b454b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 10\n",
      "Page 20\n",
      "Page 30\n",
      "Page 40\n",
      "Page 50\n",
      "Page 60\n",
      "Page 70\n",
      "Page 80\n",
      "Page 90\n",
      "Page 100\n",
      "Page 110\n",
      "Page 120\n",
      "Page 130\n",
      "Page 140\n",
      "Page 150\n",
      "Page 160\n",
      "Page 170\n",
      "Page 180\n",
      "Page 190\n",
      "Page 200\n",
      "Page 210\n",
      "Page 220\n",
      "Page 230\n",
      "Page 240\n",
      "Page 250\n",
      "Page 260\n",
      "Page 270\n",
      "Page 280\n",
      "Page 290\n",
      "Page 300\n",
      "Page 310\n",
      "Page 320\n",
      "Page 330\n",
      "Page 340\n",
      "Page 350\n",
      "Page 360\n",
      "Page 370\n",
      "Page 380\n",
      "Page 390\n",
      "Page 400\n",
      "Page 410\n",
      "Page 420\n",
      "Page 430\n",
      "Page 440\n",
      "Page 450\n",
      "Page 460\n",
      "Page 470\n",
      "Page 480\n",
      "Page 490\n"
     ]
    }
   ],
   "source": [
    "def store_embeddings(document, model, splitter):\n",
    "    for page_num, page in enumerate(document.pages):\n",
    "        if (page_num + 1) % 10 == 0:\n",
    "            print(f'Page {page_num+1}')\n",
    "        data = [\n",
    "            {\"vector\": vector, \"text\":text} for text,vector in get_embeddings(page, model, splitter)\n",
    "        ]\n",
    "        client.insert(collection_name=\"Lab_3\", data=data)\n",
    "\n",
    "store_embeddings(document, model, splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9af7e-de7e-4dfd-a044-eeace8ee3126",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.17\n",
    "\n",
    "Now that the data has been stored we can begin to perform vector searches to find text chunks that are related to a question we might pose. The `MilvusClient` object that we have connected to the Milvus container will allow us to execute queries through a `search()` method.\n",
    "\n",
    "To use the `search()` method, we need to pass in several arguments:\n",
    "\n",
    " * `collection_name` must contain the name of the collection in the database that you wish to query. For us, this is `Lab_3`.\n",
    " * `data` must contain a list of vectors that you wish to match. The vectors *must* be produced by the same model that was used to encode the vectors stored in the database or the results will make no sense. We can generate a vector by calling `model.encode()` with our question. The result of this will be a single vector, so we would need to wrap this in a `list()` to make it a list of vectors; in this case, a list of one vector.\n",
    " * `limit` is an optional kwarg that allows us to limit the number of results from the search. This allows you to choose how many of the closest matches should be returned.\n",
    " * `output_fields` is an important kwarg for us. If we do not set this to something, all of the fields are returned. For our purposes, we do not really need the ID or the vector; we only need the `text` field.\n",
    "\n",
    "Use the following cell to call `client.search()`. Query our `Lab_3` collection. Ask the question, \"What is information security?\". Use a `limit` of 5, and be sure to return only the `text` field in the output from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e13de7ba-be1f-4079-aa43-51bd295f7b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: [\"[{'id': 454178687287862215, 'distance': 0.7822209000587463, 'entity': {'text': 'information security,'}}, {'id': 454178687287862153, 'distance': 0.6972179412841797, 'entity': {'text': 'of the information security and'}}, {'id': 454178687287863743, 'distance': 0.653671145439148, 'entity': {'text': 'information on security'}}, {'id': 454178687287868247, 'distance': 0.6533844470977783, 'entity': {'text': 'information security - or privacy -related purpose.'}}, {'id': 454178687287851986, 'distance': 0.6204501986503601, 'entity': {'text': '3 The two terms information security  and security  are used synonymously  in this publication.'}}]\"] "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.search(collection_name=\"Lab_3\", data=[model.encode(\"What is information security?\")], limit=5, output_fields=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68a3d0-d557-4388-b332-f780f3c485de",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.18\n",
    "\n",
    "Looking at the returned data from `client.search()`, we can see a great deal of interesting information. Specifically, we can see the `distance` value for each chunk that has been returned. We could potentially create a threshold that could be used to filter these results so that only chunks of text meeting or exceeding that threshold could be used to build a response. We also have the `entity` field, which contains the original chunk attached to the key `text`.\n",
    "\n",
    "You can see in the cell below that we have included the `get_stream()` function that we saw in the last lab. This is the function that we used to stream the results from our Ollama container. We have also included a `query_RAG()` function that demonstrates a simple question answering prompt leveraging the text chunks returned from the vector search.\n",
    "\n",
    "Use the following cell to experiment with data returned until you are able to extract the text chunks from the result. Once you are able to isolate the texts, capture these in a list.\n",
    "\n",
    "After you are able to build the list of chunks, pass the original question and the list of chunks to the `query_RAG()` function and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ef81be-3cc3-4402-bcbc-eed6ef7afdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to my sources, information security refers to \"the information security and... information on security\". It is also mentioned that the term \"information security\" is used synonymously with \"security\", implying that they refer to the same concept."
     ]
    }
   ],
   "source": [
    "def get_stream(url, data):\n",
    "    session = requests.Session()\n",
    "\n",
    "    with session.post(url, data=data, stream=True) as resp:\n",
    "        for line in resp.iter_lines():\n",
    "            if line:\n",
    "                token = json.loads(line)[\"response\"]\n",
    "                print(token, end='')\n",
    "\n",
    "def query_RAG(question, chunks):\n",
    "    chunks = '\\n'.join(chunks)\n",
    "    prompt = f\"\"\"\n",
    "        Answer the following question using only the datasource provided. Be concise. Do not guess. \n",
    "        If you cannot answer the question from the datasource, tell the user the information they want is not\n",
    "        in your dataset. Refer to the datasource as 'my sources' any time you might use the word 'datasource'.\n",
    "\n",
    "        question: <{question}>\n",
    "\n",
    "        datasource: <{chunks}>\n",
    "        \"\"\"\n",
    "    data = {\"model\":\"llama3\", \"prompt\": prompt, \"stream\":True}\n",
    "    url = 'http://ollama:11434/api/generate'\n",
    "    get_stream(url, json.dumps(data))\n",
    "\n",
    "question = \"What is information security?\"\n",
    "result = client.search(collection_name=\"Lab_3\", data=[model.encode(question)], limit=5, output_fields=['text'])\n",
    "chunks = [i['entity']['text'] for i in result[0]]\n",
    "query_RAG(question, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9770e2-964a-4292-a11e-126f99a94147",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.19\n",
    "\n",
    "The results, so far, are not amazing. There are a variety of answers that the model might choose to generate from the source documents. One of the least impressive is:\n",
    "\n",
    "> Information security is defined as... itself.\n",
    "\n",
    "If you look at the raw chunks that were returned you can likely surmise why the results are not amazing; each chunk is pretty small! Let's see if we can improve this simply by reconfiguring the text splitter.\n",
    "\n",
    "In the following cell:\n",
    "\n",
    " * Define a new `RecursiveCharacterTextSplitter()` that uses a `chunk_size` of 400 and a `chunk_overlap` of 75.\n",
    " * Use `client.drop_collection()` to drop the `Lab_3` collection.\n",
    " * Recreate the `Lab_3` collection using `client.create_collection()`; feel free to refer to the sample code above as needed.\n",
    " * Reprocess the document using `store_embeddings()`, passing in the model and the new splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee9217f-bb53-44cb-ae00-fa72a45c1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 10\n",
      "Page 20\n",
      "Page 30\n",
      "Page 40\n",
      "Page 50\n",
      "Page 60\n",
      "Page 70\n",
      "Page 80\n",
      "Page 90\n",
      "Page 100\n",
      "Page 110\n",
      "Page 120\n",
      "Page 130\n",
      "Page 140\n",
      "Page 150\n",
      "Page 160\n",
      "Page 170\n",
      "Page 180\n",
      "Page 190\n",
      "Page 200\n",
      "Page 210\n",
      "Page 220\n",
      "Page 230\n",
      "Page 240\n",
      "Page 250\n",
      "Page 260\n",
      "Page 270\n",
      "Page 280\n",
      "Page 290\n",
      "Page 300\n",
      "Page 310\n",
      "Page 320\n",
      "Page 330\n",
      "Page 340\n",
      "Page 350\n",
      "Page 360\n",
      "Page 370\n",
      "Page 380\n",
      "Page 390\n",
      "Page 400\n",
      "Page 410\n",
      "Page 420\n",
      "Page 430\n",
      "Page 440\n",
      "Page 450\n",
      "Page 460\n",
      "Page 470\n",
      "Page 480\n",
      "Page 490\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=75,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "client.drop_collection(collection_name=\"Lab_3\")\n",
    "client.create_collection(\n",
    "    collection_name=\"Lab_3\",\n",
    "    dimension=768,\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "store_embeddings(document, model, splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0022b5-2886-4053-8f53-9a13b420c66c",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.20\n",
    "\n",
    "Using the next cell, try the question, \"What is information security?\" again. This time, ask Milvus to return the top 10 results using the `limit` argument. Build the list of chunks that are returned and then call `query_RAG()` with the question and the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d06b0d17-4748-4f2d-a21d-86693a690ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to my sources, information security refers to the protection of information and systems from unauthorized access, use, disclosure, disruption, modification, or destruction in order to provide confidentiality, integrity, and availability."
     ]
    }
   ],
   "source": [
    "question = \"What is information security?\"\n",
    "result = client.search(collection_name=\"Lab_3\", data=[model.encode(question)], limit=10, output_fields=['text'])\n",
    "chunks = [i['entity']['text'] for i in result[0]]\n",
    "query_RAG(question, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d4c3b-1b9d-4cd7-b4e4-f904e5fbdcc3",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 3.21\n",
    "\n",
    "Clearly, this is *much* better. There are more features we need to add to this to improve the results and defend our model a bit, but it's worth taking a moment to point out that we are not limited to \"questions.\"\n",
    "\n",
    "Using the following cell, bundle up the code you have created to perform the vector search, extract the chunks of text from the vector search results, and call `query_RAG()` into a function named `answer_question()`. Send the model the following:\n",
    "\n",
    "`Provide five bullet points detailing the most important password policy requirements.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "554c218c-cbd2-45de-b647-cbc149d87d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on my sources, here are five bullets detailing the most important password policy requirements:\n",
      "\n",
      "• Allow users to select long passwords and passphrases that include spaces and all printable characters.\n",
      "• Enforce organization-defined composition and complexity rules for password-based authentication (IA-5(1)(h)).\n",
      "• Verify that passwords created or updated by users are not found on lists of commonly used, expected, or compromised passwords (b).\n",
      "• Transmit passwords only over cryptographically protected channels (c), and store them using an approved salted key derivation function (d).\n",
      "• Require immediate selection of a new password upon account recovery (e).\n",
      "\n",
      "Note: These requirements are based solely on the provided text and do not include any additional information that may be necessary for comprehensive password management."
     ]
    }
   ],
   "source": [
    "def answer_question(question):\n",
    "    result = client.search(collection_name=\"Lab_3\", data=[model.encode(question)], limit=10, output_fields=['text'])\n",
    "    context = [i['entity']['text'] for i in result[0]]\n",
    "    query_RAG(question, context)\n",
    "\n",
    "question = \"Provide five bullets detailing the most important password policy requirements.\"\n",
    "answer_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083563be-8a6b-462a-b6b8-11436f80e80f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Let's reflect a bit on what we've discussed in this lab. The big take-aways are that our textual data must be converted into numbers through some means. The most common approach to this problem, especially when it comes to LLMs, is to tokenize the text into chunks and to then learn embedding vector representations of these tokens in the expectation that the various dimensions in the vectors will come to have meaningful information about the semantic and contextual meaning of each word. We also appreciate that the network really does not know what the word is or what it means, it simply knows what the vector representation of that word is.\n",
    "\n",
    "To examine these ideas we looked at two main ways of training embeddings; attempting to predict the next word given the current input word and skipgrams. We also know that the implementations of these approaches in this lab are not the only ways to implement these particular embeddings training networks.\n",
    "\n",
    "With these understandings, we are now ready to start looking at how to preprocess our existing text data in such a way as to make it useful for various AI techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
