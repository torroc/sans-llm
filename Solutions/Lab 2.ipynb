{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d9877102",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\\pagenumbering{gobble}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba88514",
   "metadata": {},
   "source": [
    "# Lab 2: Exploring Ollama\n",
    "\n",
    "## Overview\n",
    "Many of our labs will require the use of an LLM. Rather than using an online hosted commercial LLM with all of the associated fees, we will use a containerized version of Ollama serving a 3 billion parameter model.\n",
    "\n",
    "## Goals\n",
    "\n",
    " * Pull and serve the Llama3 model in an Ollama container.\n",
    " * Understand how to use the HTTP API to interact with the model.\n",
    " * Develop an understanding of how context drives chatbots.\n",
    "\n",
    "## Estimated Time: 60 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd729486",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.1\n",
    "\n",
    "## Pulling a Trained Model\n",
    "\n",
    "When you issued the initial `docker compose up` command, several different services were started. One of those is the Jupyter system through which you are interacting with these exercises. Another set of services supports a *vector store* that we will be using later in the class. The last is a container that is hosting and serving Ollama.\n",
    "\n",
    "Ollama is an open source project under the MIT license design to host and serve various open LLMs. In this course, we will make use of the `llama3` model, but feel free to experiment with any of them. We have chosen this particular model because it is small enough to fit within the contraints of the system requirements that were specified for this course.\n",
    "\n",
    "When the container is first started, it is ready to work but has no model loaded. We need to interact with it a bit to instruct Ollama to `pull` the `llama3` model. To do this we will interact with the HTTP API that it provides on port 11434.\n",
    "\n",
    "To begin with, we need to load some Python libraries so that we can issue HTTP API calls easily. Please import the `requests` and the `json` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28985ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e924083",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.2\n",
    "\n",
    "Let's start by verifying that Ollama is reachable. Normally we would need to either have Ollama running locally, know its IP address, or know its fully qualified domain name. In our cases, since we are running all of these containers together, we can take advantage of the automatic naming that containerization solutions provide.\n",
    "\n",
    "Using the host name `ollama`, send an HTTP GET request to that host on port `11434` and examine the returned content. This can be done using the `requests.get()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129f1f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://ollama:11434').content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d512fe-9baa-41db-9a67-a878877554ab",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.3\n",
    "\n",
    "The `Ollama is running` response tells us that Ollama is up and running, however there's still another step that must be taken. Ollama provides a platform that can download and serve a number of different models. While Ollama is running, we have not downloaded any models.\n",
    "\n",
    "Let's prove this and demonstrate how to send data to the API. Since we are sending data, we must use an HTTP `POST` rather than a `GET`, which we just did. To send a `POST` we can use `requests.post()`. This will, however, require a few more arguments:\n",
    "\n",
    " * We must configure the *request headers* to specify the data type we are sending and that we wish to receive. This should be a dictionary containing `{'Content-Type':'application/json'}`.\n",
    " * We must also send a JSON body. This can also be built as a Python dictionary with the following keys and values:\n",
    "   - A `model` key with the value `llama3`, which is the model we wish to use. This parameter allows us to select the model used to generate responses.\n",
    "   - A `prompt` key with the text or prompt we want completed. Let's use `What is 42?`.\n",
    "   - A `stream` key with the value `False`. This key allows us to control whether the response is returned as a single response or streamed as individual tokens are generated by the model. For now, let's be patient and wait for the entire response.\n",
    "\n",
    "Our request must also be sent to a different API endpoint. To ask a model to generate text, the URL we must use is `http://exercises-ollama-1:11434/api/generate`.\n",
    "\n",
    "Please use the empty cell below to generate a `POST` request to the Ollama container. Use the `requests.post()` method, passing the URL, the headers and the data. The headers should be passed using kwarg `headers` and the data should be sent using kwarg `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8871fe1c-0eb0-4f6e-aa05-cee662023dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"error\":\"model \\\\\"llama3\\\\\" not found, try pulling it first\"}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {\"model\":\"llama3\", \"prompt\": \"What is 42?\", \"stream\":False}\n",
    "requests.post('http://ollama:11434/api/generate', headers=headers, json=data ).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe89f96-81b4-48f8-b476-ba4aa8c853c9",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.4\n",
    "\n",
    "As predicted, the server reports that we do not have a model loaded with the message, `b'{\"error\":\"model \\\\\"llama3\\\\\" not found, try pulling it first\"}'`.\n",
    "\n",
    "To tell Ollama to pull the model, we must use the `/api/pull` API endpoint. To use this endpoint, we must configure the data that we send with the name of the model to pull.\n",
    "\n",
    "Use the following cell to send a `POST` request to the `/api/pull` endpoint. This time, the `data` parameter should be configured as:\n",
    "\n",
    "`data = {\"name\":\"llama3\", \"stream\":False}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63b37ea0-e864-45eb-b3cd-ac6f7541f509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"status\":\"success\"}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"name\":\"llama3\", \"stream\":False}\n",
    "url = 'http://ollama:11434/api/pull'\n",
    "requests.post(url, headers=headers, json=data).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18490d-179f-4123-958a-fa624ebce1e2",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.5\n",
    "\n",
    "Running this cell will require some patience. In fact, if you watch the command line from which you ran `docker compose up` you will see Ollama messages detailing the download progress. Please be patient. Depending on your Internet connection speed, this could take several minutes to complete. Once it does complete, you should see the final status message indicating `\"success\"`.\n",
    "\n",
    "With the model now downloaded, we should be able to send a query. Please resend the same request from **Task 2.3**. Capture the `.content` of the request in a variable named `response`. (For example, `response = requests.post(URL).content`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312c1676-696c-40b1-bdc6-be639729cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"model\":\"llama3\", \"prompt\": \"What is 42?\", \"stream\":False}\n",
    "response = requests.post('http://ollama:11434/api/generate', headers=headers, json=data ).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d3247-ea23-47fe-b7af-c1cf6506dcfd",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.6\n",
    "\n",
    "This cell may take 30 seconds or more to run. When it completes, you will see the asterisk turn into a number, indicating completion, but you should not see any output since we captured the content of the result into a variable. Please execute the following cell to examine the content returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc91529-49f1-4dad-a8be-f497775077e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"model\":\"llama3\",\"created_at\":\"2024-11-25T17:22:31.973293664Z\",\"response\":\"The answer to the ultimate question of life, the universe, and everything!\\\\n\\\\nAccording to Douglas Adams\\' science fiction series \\\\\"The Hitchhiker\\'s Guide to the Galaxy,\\\\\" 42 is the \\\\\"Answer to the Ultimate Question of Life, the Universe, and Everything.\\\\\" In the book, a group of hyper-intelligent beings build a massive supercomputer named Deep Thought to find the answer to the ultimate question. After seven years of calculations, Deep Thought reveals that the answer is indeed 42.\\\\n\\\\nHowever, the characters in the story then realize that they don\\'t actually know what the ultimate question is, so the answer is essentially meaningless! Despite this, the number 42 has become a popular meme and cultural reference, often used to humorously signify an elusive or mysterious answer.\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,271,3923,374,220,2983,30,128009,128006,78191,128007,271,791,4320,311,279,17139,3488,315,2324,11,279,15861,11,323,4395,2268,11439,311,31164,27329,6,8198,17422,4101,330,791,71464,71,25840,596,13002,311,279,20238,1359,220,2983,374,279,330,16533,311,279,29950,16225,315,9601,11,279,29849,11,323,20696,1210,763,279,2363,11,264,1912,315,17508,20653,21149,23837,1977,264,11191,2307,44211,7086,18682,36287,311,1505,279,4320,311,279,17139,3488,13,4740,8254,1667,315,29217,11,18682,36287,21667,430,279,4320,374,13118,220,2983,382,11458,11,279,5885,304,279,3446,1243,13383,430,814,1541,956,3604,1440,1148,279,17139,3488,374,11,779,279,4320,374,16168,57026,0,18185,420,11,279,1396,220,2983,706,3719,264,5526,42285,323,13042,5905,11,3629,1511,311,28485,7162,89522,459,66684,477,26454,4320,13],\"total_duration\":3666922509,\"load_duration\":2287080528,\"prompt_eval_count\":15,\"prompt_eval_duration\":19118000,\"eval_count\":155,\"eval_duration\":1313824000}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a75a8-8e35-4965-ab50-e792d8efa4f1",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.7\n",
    "\n",
    "First, the exact response that you receive may be different from the result shown above in the solutions notebook. This is because there is a bit of randomness added to the next-word result in the model. Take a few moments to examine the response. You should be able to find the following things:\n",
    "\n",
    " * `model` key, indicating this result is from `llama3`.\n",
    " * `created_at` key, telling you when the response was generated.\n",
    " * `response` key, providing the complete response as a string.\n",
    " * `done` key, indicating that the response is complete.\n",
    " * `done_reason` key, telling us why the model stopped.\n",
    " * `context` key, providing a list of the token indices including the prompt and the response.`\n",
    " * `total_duration` key, indicating the number of nanoseconds spent generating the resopnse.\n",
    " * `load_duration` key, indicating the number of nanoseconds spent loading the model.\n",
    " * `prompt_eval_count` key, the number of tokens in the prompt.\n",
    " * `prompt_eval_duration` key, the time in nanoseconds spent evaluating the prompt.\n",
    " * `eval_count` key, indicating the total number of tokens sent in the response.\n",
    " * `eval_duration` key, detailing the number of nanoseconds spent generating the response.\n",
    "\n",
    "To make the response easier to work with, let's convert it into a Python dictionary. This can be done using the `json.loads()` function.\n",
    "\n",
    "Use the next cell to decode the `response` into a Python dictionary named `response`. Once this is done, print out the `'response'` key from this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdeac5e9-bb34-48de-a5cd-6dec77200002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to the ultimate question of life, the universe, and everything!\n",
      "\n",
      "According to Douglas Adams' science fiction series \"The Hitchhiker's Guide to the Galaxy,\" 42 is the \"Answer to the Ultimate Question of Life, the Universe, and Everything.\" In the book, a group of hyper-intelligent beings build a massive supercomputer named Deep Thought to find the answer to the ultimate question. After seven years of calculations, Deep Thought reveals that the answer is indeed 42.\n",
      "\n",
      "However, the characters in the story then realize that they don't actually know what the ultimate question is, so the answer is essentially meaningless! Despite this, the number 42 has become a popular meme and cultural reference, often used to humorously signify an elusive or mysterious answer.\n"
     ]
    }
   ],
   "source": [
    "response = json.loads(response)\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9c681-51ee-43f7-8812-84d5ec77d12e",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.8\n",
    "\n",
    "Wonderful! While the response is not fast, we are able to send queries and have them answered. What about speed?\n",
    "\n",
    "First, we will make no attempt to speed things up in this class. The reason that the model response seems slow is twofold. First, we have not done anything to attempt to get any GPUs in the system properly configured, nor have we attempted (nor will we) to install GPU driver support into Docker or Kubernetes. If your organization is planning to deploy this type of model you should *definitely* investigate which GPUs make the most sense for your applications, your platforms (containerized or not), and your systems.\n",
    "\n",
    "The second reason this seems so slow is that we do not see anything until the entire response has been generated. To improve our experience during class (and for any interactive chat app you might build), let's change how we're making the request.\n",
    "\n",
    "The `\"stream\"` option in the JSON request, when set to `True`, will stream chunks of the response (tokens) as they become available. This sounds much more pleasant, but it requires a bit of a different approach in our Python code.\n",
    "\n",
    "Please consider the Python code in the cell below and, when you have a good handle on what it is doing, execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac85495-7717-4b2d-b9e8-eab70f50f8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not a Large Language Model (LLM) myself, but rather a smaller AI model trained on a specific domain or task. I'm an AI designed to simulate conversation and answer questions to the best of my ability based on my training data.\n",
      "\n",
      "However, if you're asking about which LLMs are out there, there are many impressive ones that have been developed in recent years! Some notable examples include:\n",
      "\n",
      "1. DALL-E: A text-to-image model that generates images from natural language prompts.\n",
      "2. BERT (Bidirectional Encoder Representations from Transformers): A pre-trained language model developed by Google that can be fine-tuned for a wide range of NLP tasks.\n",
      "3. T5 (Text-to-Text Transformer): Another pre-trained language model developed by Google that can perform various text-based tasks, such as language translation and generation.\n",
      "4. transformer-XL: A long-range dependence version of the original transformer architecture, designed to process longer input sequences.\n",
      "5. GPT-3 (Generative Pre-training Technique-3): A large-scale language model developed by Meta AI that can generate human-like text and perform various NLP tasks.\n",
      "\n",
      "These are just a few examples, but there are many more LLMs out there, each with its unique capabilities and applications!"
     ]
    }
   ],
   "source": [
    "def get_stream(url, data):\n",
    "    session = requests.Session()\n",
    "\n",
    "    with session.post(url, data=data, stream=True) as resp:\n",
    "        for line in resp.iter_lines():\n",
    "            if line:\n",
    "                token = json.loads(line)[\"response\"]\n",
    "                print(token, end='')\n",
    "\n",
    "data = {\"model\":\"llama3\", \"prompt\": \"Which LLM are you?\", \"stream\":True}\n",
    "url = 'http://ollama:11434/api/generate'\n",
    "\n",
    "get_stream(url, json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5131380-0311-4cbd-ac6b-7a354ba34566",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.9\n",
    "\n",
    "Wow, that's much better! It's still taking the model a while to generate the answer, but the delay is much more tolerable since we can see what it is doing. Before concluding this lab, let's investigate the `\"context\"` value and see how it can be used. First:\n",
    "\n",
    "Using the next cell and the techniques above, ask the model, \"Who was Macbeth?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2a0cf6-4595-4b66-8552-eab768df865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macbeth is a legendary Scottish nobleman and the titular character of William Shakespeare's tragic play, \"Macbeth\". He is also known as King Macbeth or Mac Bethad mac Findláich.\n",
      "\n",
      "In reality, there are different accounts of Macbeth's life, making it challenging to separate fact from fiction. The most widely accepted account comes from medieval Scottish records and chronicles, particularly the \"Scots Magazine\" and \"The Historie of Scotland\".\n",
      "\n",
      "Macbeth was a thane (nobleman) in the court of King Duncan I of Scotland. According to historical records, Macbeth lived around 1005-1057 AD, during the reign of Máel Coluim mac Cináeda (Malcolm II), who ruled Scotland from 1005 to 1034.\n",
      "\n",
      "Shakespeare's play is largely fictionalized and based on a brief mention in Holinshed's Chronicles, a 16th-century English history book. The play tells the story of Macbeth, a Scottish nobleman who, after encountering three witches prophesying his future, becomes consumed by ambition and greed, leading him to commit regicide (murder of a king) and seize the throne.\n",
      "\n",
      "In reality, there is no conclusive evidence that Macbeth actually murdered King Duncan or became king. Some historians believe that Macbeth may have been involved in the death of King Duncan's son, Malcolm II, but this is also uncertain.\n",
      "\n",
      "Macbeth's legacy has become entwined with Shakespeare's play, making him a legendary figure in Scottish and English folklore."
     ]
    }
   ],
   "source": [
    "data = {\"model\":\"llama3\", \"prompt\": \"Who was Macbeth?\", \"stream\":True}\n",
    "url = 'http://ollama:11434/api/generate'\n",
    "\n",
    "get_stream(url, json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7aad9-f4a0-4138-b688-457ba5300302",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.10\n",
    "\n",
    "That response seems completely reasonable. In the event you are looking at the solution while working through this on your own, do not be concerned if the response generated by your model is not identical. No doubt it includes the highlights; specifically, something about Macbeth being a fictional Shakespearean character based on a real historical figure.\n",
    "\n",
    "Using the next cell and the techniques above, ask the model, \"What did the witches say about him?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3ebd23-ce9a-47c2-b7d7-b8f5ba02ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help you with your question, but I need a bit more context. Which witches are you referring to, and what was said about whom? Could you provide more information or clarify your question? I'll do my best to help you out!"
     ]
    }
   ],
   "source": [
    "data = {\"model\":\"llama3\", \"prompt\": \"What did the witches say about him?\", \"stream\":True}\n",
    "url = 'http://ollama:11434/api/generate'\n",
    "\n",
    "get_stream(url, json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e454e-8440-4f5b-8e3f-490468f2df1b",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.11\n",
    "\n",
    "What happened? The model acts like it has no idea what we are talking about!\n",
    "\n",
    "The problem is that every prompt that we send to the model is viewed as a completely discrete event. Unless we do something to remind the model about the history of our conversation, it will have no way to connect the second question to the first, resulting in a response that isn't particularly useful. This brings us to the `\"context\"` field.\n",
    "\n",
    "The context is a list of tokens that the model returns to us, providing us information in the form of token numbers about the question and the response that the model generates. If we store this value from the response to our first question and then send it in our second question, the model will perform as we might expect. Let's try it.\n",
    "\n",
    " * Redefine the `get_stream()` function such that it returns the `'context'` array from the JSON object in the last part of the stream.\n",
    " * Capture this value in a variable\n",
    " * Use this new function to re-send the question, \"Who was Macbeth?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d37b65e-1c94-4c24-b33d-f651f1e761c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macbeth is the titular character in William Shakespeare's tragedy \"Macbeth.\" He was a Scottish nobleman and general who, after encountering three witches who foretold his future, becomes consumed by ambition and greed. The play tells the story of how he rises to power and commits regicide and other atrocities to become king of Scotland.\n",
      "\n",
      "According to historical records, Macbeth was a real-life Scottish nobleman who lived in the 11th century. He was Thane (or Earl) of Moray and Rosemarkie, and played an important role in the Battle of Lostwithiel in 1054.\n",
      "\n",
      "In Shakespeare's play, however, Macbeth is portrayed as a fictional character whose actions are driven by his own ambition and guilt. The story is set in medieval Scotland and explores themes of power, morality, and the supernatural.\n",
      "\n",
      "The play begins with Macbeth and his friend Banquo, another Scottish nobleman, encountering three witches who foretell their futures. The witches predict that Macbeth will become thane of Cawdor and eventually king of Scotland, while Banquo's descendants will rule Scotland for generations to come. Driven by ambition and spurred on by the prophecies, Macbeth murders King Duncan and seizes the throne.\n",
      "\n",
      "However, his reign is marked by guilt, paranoia, and violence, as he becomes increasingly tyrannical and isolated. In the end, Macbeth's own downfall is brought about by his own actions, as well as those of Lady Macbeth, his wife, who plays a significant role in goading him to commit the murder.\n",
      "\n",
      "Overall, Shakespeare's Macbeth is a complex and nuanced character whose tragic fall from power serves as a cautionary tale about the dangers of unchecked ambition and the corrupting influence of power."
     ]
    }
   ],
   "source": [
    "def get_stream(url, data):\n",
    "    session = requests.Session()\n",
    "\n",
    "    with session.post(url, data=data, stream=True) as resp:\n",
    "        for line in resp.iter_lines():\n",
    "            if line:\n",
    "                token = json.loads(line)[\"response\"]\n",
    "                print(token, end='')\n",
    "        return json.loads(line)['context']\n",
    "\n",
    "data = {\"model\":\"llama3\", \"prompt\": \"Who was Macbeth?\", \"stream\":True}\n",
    "url = 'http://ollama:11434/api/generate'\n",
    "\n",
    "context = get_stream(url, json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062aa71e-9b6d-4383-9024-eadee4ff591f",
   "metadata": {},
   "source": [
    "# <img src=\"../images/task.png\" width=20 height=20> Task 2.12\n",
    "\n",
    "Now that we have the initial answer and the context, we are ready to ask the second question. We just have to remember to send the context value in the `data` object.\n",
    "\n",
    " * Add a `\"context\"` key to the `data` dictionary with the context array that was returned in the last cell.\n",
    " * Ask the model, \"What did the witches say about him?\", sending the context in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c7ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In William Shakespeare's play \"Macbeth\", the three witches, also known as the Weird Sisters, reveal several prophecies to Macbeth that shape his fate and drive the plot forward. Here are some key things they say about Macbeth:\n",
      "\n",
      "1. **Thane of Cawdor**: The first prophecy is that Macbeth will become Thane of Cawdor, which is a significant title in Scotland at the time. This prophecy is fulfilled when King Duncan appoints Macbeth to the position.\n",
      "2. **King of Scotland**: The witches predict that Macbeth will become king of Scotland, but they don't specify how this will happen. They say: \"All hail, Macbeth, thou shalt be king!\" (Act 1, Scene 1). This prophecy drives Macbeth's ambition and ultimately leads him to murder King Duncan.\n",
      "3. **Banquo's descendants**: The witches also predict that Banquo's children will become kings of Scotland, but they won't be killed by Macbeth as he is. Instead, Macbeth becomes increasingly paranoid about the possibility of being overthrown by Banquo's descendants.\n",
      "4. **Macduff's fate**: The witches reveal that Macduff, a Scottish nobleman and opponent of Macbeth, will not be killed until \"great Birnam wood\" comes to Dunsinane Hill, where Macbeth resides. This prophecy is fulfilled when Macduff leads an army to besiege Dunsinane, using branches from Birnam Wood as camouflage.\n",
      "5. **Macbeth's own downfall**: The witches hint at Macbeth's eventual downfall, saying: \"None of woman born shall harm Macbeth\" (Act 4, Scene 1). This prophecy is fulfilled when Macduff, who was born by cesarean section, kills Macbeth.\n",
      "\n",
      "The witches' prophecies are intentionally ambiguous and open-ended, allowing Macbeth to interpret them in various ways. However, as the play progresses, it becomes clear that the witches are simply revealing the consequences of Macbeth's own actions, rather than dictating his fate."
     ]
    }
   ],
   "source": [
    "data = {\"model\":\"llama3\", \"prompt\": \"What did the witches say about him?\", \"stream\":True, \"context\":context}\n",
    "url = 'http://ollama:11434/api/generate'\n",
    "\n",
    "context = get_stream(url, json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083563be-8a6b-462a-b6b8-11436f80e80f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab we have accomplished some important things and learned some useful techniques:\n",
    "\n",
    " * We now have the Llama 3 model installed in our Ollama container.\n",
    " * We know how to interact with the API to pull models.\n",
    " * We know how to interact with the API to send questions.\n",
    " * We understand the function of the `stream` attribute and have code that allows us to receive and print out each part of the response as it arrives.\n",
    " * We understand how the `context` is returned and how it can be included in a subsequent query to continue the \"conversation.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
